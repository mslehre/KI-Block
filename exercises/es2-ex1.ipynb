{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95723ba3-57b4-4c92-bcd8-1b1403365be8",
   "metadata": {},
   "source": [
    "<sub>Felix Becker, Lars Gabriel University of Greifswald, Germany</sub>\n",
    "# Exercise Set 2 - 1: A simple neural network for bike rentals\n",
    "\n",
    "We will now create a simple neural network that predicts rental counts based on different attributes as defined in the first exercise (matrix $X$).\n",
    "\n",
    "We will also learn how to optimize relevant hyperparameters of the model without underestimating the true test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2c6eb6e-3406-4528-b4bb-9f4a3318d4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2cb9c8-6c67-404c-a0c9-16f1ab8f0136",
   "metadata": {},
   "source": [
    "## Task 1: Preparations\n",
    "\n",
    "Shuffle with `np.random.shuffle` and split X and y into ``X_train``, ``X_test``, ``y_train`` and ``y_test``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e70e122-a17e-4b96-98ba-43869bfa8028",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0) # so we all get the same pseudorandom results, do not modify or remove this line!\n",
    "\n",
    "#load X and y\n",
    "X = np.load(\"solutions/X.npy\")\n",
    "y = np.load(\"solutions/y.npy\")\n",
    "\n",
    "\n",
    "m,n = X.shape\n",
    "\n",
    "#split into train and test\n",
    "test_split = 0.1\n",
    "#YOUR CODE STARTS\n",
    "\n",
    "#shuffle X and y (attention, shuffle them together, i.e. keep the one-one correspondance of rows of X and values in y)\n",
    "#split X and y by keeping the first (test_split*100)% examples for testing. Use the rest for training.\n",
    "#Name the splitted values X_train, X_test, y_train and y_test.\n",
    "#use this permutation vector:\n",
    "perm = np.arange(m)\n",
    "np.random.shuffle(perm) #shuffle all data examples\n",
    "\n",
    "#replace these lines\n",
    "X_train, X_test = X, X\n",
    "y_train, y_test = y, y\n",
    "\n",
    "#YOUR CODE ENDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec72088-6cfd-4102-bc26-e444784fa59e",
   "metadata": {},
   "source": [
    "## Task 2: Model search\n",
    "\n",
    "0. Run all cells until Task 3 as they are (without adding your own code yet). This trains a linear regression (i.e. a neural network without hidden layers) as a baseline model.\n",
    "1. Create a small neural network with hidden layers of various sizes and an activation function of your choice.\n",
    "2. Add a L2 regularizer with ``kernel_regularizer=tf.keras.regularizers.L2(l2)`` to each Dense layer you create.\n",
    "3. After training, note down *validation* error, come back here, change some hyperparameters and train again. Iterate as much as you like. Also adapt the l2 loss strength to a value > 0.\n",
    "4. Report (as a group) the final *test* error of the best model and its configuration. Who wins? What is the improvement with respect to the baseline model (linear regression)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db043335-962a-456f-9b07-c344e5c2ba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0) # so we all get the same pseudorandom results, do not modify or remove this line!\n",
    "\n",
    "l2 = 0 #strength of l2 regularization for all kernels\n",
    "\n",
    "def make_model():\n",
    "    # a neural network is a stack of layers, in keras called \"Sequential\" model\n",
    "    model = tf.keras.models.Sequential() # so far the stack is empty, 0 layers\n",
    "\n",
    "    #YOUR CODE STARTS\n",
    "    # add Dense neural network layers with number of hidden neurons, activation function of your choice.\n",
    "    # add a L2 regularizer for each layer\n",
    "    #YOUR CODE ENDS\n",
    "    \n",
    "    #last layer maps to output size 1 and has no activation function\n",
    "    model.add(tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.L2(l2)))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = make_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55dc330-992c-4658-8469-199303666d9c",
   "metadata": {},
   "source": [
    "In addition to our loss (MSE) we use the **Root Mean Squared Error** as a metric.\n",
    "\n",
    "$RMSE(\\theta) = \\sqrt(MSE(\\theta))$\n",
    "\n",
    "It can be interpreted as the average distance of our prediction to the true value.\n",
    "\n",
    "A **metric** is (in constrast to a **loss**) not used for model optimization but for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16dd5b14-0abb-4d58-9be9-9742c83b3e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, X_train, y_train, epochs=100, learning_rate = 0.01):\n",
    "    # define the loss, optimization algorithm and prepare the model for gradient computation \n",
    "    opti = tf.keras.optimizers.Adam(learning_rate = learning_rate) # Adam is a popular method for stochastic gradient descent\n",
    "    model.compile(optimizer = opti, loss = \"mse\", metrics = [tf.keras.metrics.RootMeanSquaredError()]) \n",
    "    # execute the actual training \n",
    "    val_split = 0.15\n",
    "    history = model.fit(X_train, y_train, epochs = epochs, batch_size=64, validation_split=val_split, verbose = 0) \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1b374bf-8e98-4b00-a82a-424bbbcd6092",
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes ~1min\n",
    "#a larger batch size can speed this up, however it can negatively affect the outcome since fewer training steps are made\n",
    "#the same is true for smaller numbers of epochs\n",
    "history = fit_model(model, X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68581d15-1915-4c5c-bc96-7048f061ffc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax= plt.subplots(1,1)\n",
    "def plot_hist(history, ax, title):\n",
    "    ax.plot(history.history[\"root_mean_squared_error\"], label=\"training\") \n",
    "    ax.plot(history.history[\"val_root_mean_squared_error\"], label=\"validation\") \n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"training epoch\")\n",
    "    ax.set_ylabel(\"root mean squared error\")\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, labels)\n",
    "plot_hist(history, ax, \"training and validation loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987fab45-9b6a-487d-8fff-a2f860c42923",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"val mse=\", history.history[\"val_root_mean_squared_error\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f986e5e7-79a0-4db3-8241-74fb4af79462",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute test error\n",
    "l = model.evaluate(X_test, y_test)\n",
    "print(\"test mse, rmse=\", l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acb3ecc-96d8-419e-a5fd-c8fdd55634be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"predictions =\\t\", model.predict(X_test).flatten().astype(np.int32))\n",
    "print(\"truth =\\t\\t\", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebf66f7-895e-42fb-98d1-dd84e05b6b6a",
   "metadata": {},
   "source": [
    "Iterate at least until you have a lower validation rmse than with the linear regression baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ff08d3-3b56-4bde-9992-875c6994676c",
   "metadata": {},
   "source": [
    "## Task 3: Transfer learning\n",
    "\n",
    "We now have a well working neural network architecture and a trained ``model`` at hand. Naturally, as a black box, we can hardly tell anything about the inner workings of the neural network, i.e. we see that a prediction is good, but can't tell why.\n",
    "\n",
    "However, we expect the inner layers of the neural network to learn *something* about the semantics of the problem space at hand.\n",
    "\n",
    "To see this, assume we now have a different training target that is related to the original one. The dataset contains a \"registered\" column that counts only those rentals, with a registration (club membership). \"registered\" is similar to count, but not the same because it misses casual rentals. Maybe we can expect casual rentals to be higher in summer and spring and expect registered members to use the service more regularily over the year. \n",
    "\n",
    "*Question: Can we use the hidden features of the already trained model for \"count\" to train a model for \"registered\"?*\n",
    "\n",
    "To answer this, you will train two models in the following: \n",
    "1. ``model1`` is identical to the original model except the last layer (Dense(1))\n",
    "2. ``model2`` is completely new\n",
    "\n",
    "To achieve this, use the trained ``model`` from previous cells and access its layer list with ``model.layers``. Reuse these layers for the new model (``.add``) and remember to leave out the last layer. Replace it with a new one and also apply L2 regularization to it.\n",
    "\n",
    "Use ``fit_model`` to train both models on ``y_registered_train`` and try fewer epochs (e.g. 200) for model1 as it already has trained hidden features.\n",
    "\n",
    "Name the two return values of ``fit_model`` ``history1`` and `history2``.\n",
    "\n",
    "How does the loss curve change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82a19823-abbe-45ac-8339-5e1a574952e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_registered = pd.read_csv(\"../bikes-summerdays-full.csv\", sep = \";\")[\"registered\"]\n",
    "y_registered_train = y_registered[perm[k:]]\n",
    "y_registered_test = y_registered[perm[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9982ac88-0b30-4014-bbb8-968232cdf74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE STARTS\n",
    "#construct model1 and model2 as described above, train them and name the results history1 and history2\n",
    "#YOUR CODE ENDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645beee8-45a8-4f05-9c8b-75b808e236c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2)\n",
    "plot_hist(history1, axes[0], \"model1\")\n",
    "plot_hist(history2, axes[1], \"model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e67be92-d731-4c3b-b21b-d16380d7d2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test mse 1=\", model1.evaluate(X_test, y_registered_test, verbose=0))\n",
    "print(\"test mse 2=\", model2.evaluate(X_test, y_registered_test, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9dc70a-891b-443c-887b-54a2a719e7ec",
   "metadata": {},
   "source": [
    "$\\implies$ model 1 should achieve similar (or sometimes better) validation errors and training converges much faster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ed7dca-7d5f-4226-960f-66db5a3a7e6a",
   "metadata": {},
   "source": [
    "### Improve the model search \n",
    "\n",
    "From here, if you want to practice off-course more or want to apply neural networks to your own data, you should consider averaging the test error over many different training-validation-test splits of the data. This is called **cross-validation** and it will trade experiment time for decreased randomness in the reported result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664f7646-ae60-4257-a443-4308a2c6591c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
