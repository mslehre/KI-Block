{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"text-align:right;\">Mario Stanke, University of Greifswald, Germany</p>\n",
    "\n",
    "# Exercise Set 1 - 2 - SGD Parameters\n",
    "\n",
    "Play with the parameters of the stochastic gradient descent algorithm.\n",
    " 1. Execute the notebook with the high-level method from this morning's lecture as is (Kernel -> Restart & Run All).\n",
    " 2. Jump to **Exercise** after the heading \"Solution 2\" and do as told there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m =  1482 \tn_cols =  2\n"
     ]
    }
   ],
   "source": [
    "# load the training data from the table file to a pandas data frame\n",
    "df = pd.read_csv(\"bikes-summerdays.tbl\", sep='\\s+')\n",
    "\n",
    "# convert count data to floats as regression target\n",
    "df['count'] = df['count'].astype(float) \n",
    "\n",
    "m, n_cols = df.shape # training set size and number of columns \n",
    "print(\"m = \", m, \"\\tn_cols = \", n_cols)\n",
    "\n",
    "# compute average of hourly bike rental counts\n",
    "meancount = np.mean(df['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Matrix $X$ and response vector $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract response vector\n",
    "y_train = np.array(df['count']) \n",
    "\n",
    "# extract feature columns\n",
    "n = n_cols - 1 # number of features\n",
    "temps = np.array(df.loc[:, df.columns != 'count']) # matrix of all other columns (here only one col)\n",
    "\n",
    "# make data matrix X\n",
    "X_train = np.ones((m, n+1)) # initialize with all ones\n",
    "# overwrite all but the zero-th column with features\n",
    "X_train[:,1:n+1] = temps / 10 - 2.5 # normalize temperatures so they are roughly in [-1,1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean squared error loss function\n",
    "def MSE(theta):\n",
    "    \"\"\"\n",
    "        Mean squared error function for linear regression\n",
    "        theta: numpy array of parameters, the first dimension must match the number of cols of X\n",
    "               If theta is 2-dimensional, the output is 1-dim with one entry per col of theta.\n",
    "    \"\"\"\n",
    "    if len(theta.shape) == 1: # theta is 1-dimensional\n",
    "        theta = tf.reshape(theta, (-1, 1)) # make it a matrix with one column\n",
    "    # now theta is a matrix in any case\n",
    "    yhat = tf.linalg.matmul(X_train, theta) # vector of predicted rental counts\n",
    "    d = (yhat - y_train.reshape((-1, 1)))**2 # square the residuals ('errors')\n",
    "    E = tf.reduce_sum(d, axis=0) / m\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two tests of calls to error function\n",
    "print(\"single point:\", MSE(np.array([1., 2.])))\n",
    "print(\"two points: \", MSE(np.array([[1., 3.], [2., 4.]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# this is for lecture purposes only and NEED NOT BE READ\n",
    "def contourPlot():\n",
    "    ''' Plot the error landscape'''\n",
    "    # compute error for the grid of all combinations of theta0 and theta1 values\n",
    "    theta0 = theta1 = np.arange(-3200., 3200., 100) # grid axis ranges\n",
    "    xv, yv = np.meshgrid(theta0, theta1) # x and y values of all grid points\n",
    "    Theta = np.array([xv, yv]).reshape(2, -1) # 2 rows, one col per grid point\n",
    "\n",
    "    # compute error for all grid points\n",
    "    z = MSE(Theta).numpy()\n",
    "    z = z.reshape((theta0.size, -1)) # make this a matrix again as required by contour\n",
    "\n",
    "    ## make contour plot\n",
    "    _, axC = plt.subplots()\n",
    "    # heights to draw contour lines for\n",
    "    h = [50000, 200000] + list(range(500000, 6000001, 500000))\n",
    "    contours = axC.contour(theta0, theta1, z, levels=h, colors='black')\n",
    "    axC.clabel(contours, inline=True, fontsize=8, fmt='%i') # labels on lines\n",
    "    plt.title(\"mean squared error \" + r'$E(\\theta)$');\n",
    "    plt.xlabel(r'$\\theta_0$')\n",
    "    plt.ylabel(r'$\\theta_1$');\n",
    "    return axC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of optimization progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error(err, col):\n",
    "    ''' Plot error over time'''\n",
    "    plt.subplots()\n",
    "    plt.plot(np.log(err), 'o-', color=col, linewidth=.2, markersize=3, mfc='none')\n",
    "    plt.title(\"Log error by iteration of gradient descent\")\n",
    "    plt.xlabel('iteration '+r'$i$')\n",
    "    plt.ylabel(r'$ln \\;E(\\theta^{(i)})$');\n",
    "\n",
    "def plot_progress(Theta, err, col, ):\n",
    "    '''Plot error and parameters over time'''\n",
    "    plot_error(err, col)\n",
    "    # enter parameter trajectory to above contour plot\n",
    "    axC = contourPlot()\n",
    "    plt.plot(Theta[0, 0], Theta[1, 0], 'o', color='green', mfc='none') # mark starting point with circle\n",
    "    ymin, ymax = axC.get_ylim()\n",
    "    xmin, xmax = axC.get_xlim()\n",
    "    for i in range(1, Theta.shape[1]):\n",
    "        if (np.abs(Theta[0, i] - Theta[0, i-1]) + np.abs(Theta[1, i] - Theta[1, i-1]) > 1e-6):\n",
    "            # above condition avoids arrows with a length of 0 pixels\n",
    "            plt.arrow(Theta[0, i-1], Theta[1, i-1], Theta[0, i] - Theta[0, i-1], Theta[1, i] - Theta[1, i-1],\n",
    "                      color=col, width=3, head_length=20, head_width=50, overhang=.9, length_includes_head=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 2: high-level approach\n",
    "\n",
    "**Exercise:**  \n",
    "  1. Play with the *learning rate* $\\alpha$. Find a *small* value and a *large* value so that **convergence is not reached**.\n",
    "  2. Play with the *momentum* $\\mu$. Find a *small* value and a *large* value so that **convergence is not reached**.\n",
    "  3. Play with ```batch_size```. Find a *small* value so training is significantly **slower** and a *large* value so **convergence is not reached**.\n",
    "  \n",
    "When you vary one parameter, leave the other parameters as they were originally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classic Option 1: Use the **keyboard**\n",
    "to edit the values in below cell, then **Run -> Run Selected Cell and Below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THIS CODE HERE for Option 1. If you choose Option 2, leave these values as defaults (0.01, 0.8 and 32, respectively)\n",
    "alpha = 0.01    # Solutions: ? and ?\n",
    "mu = 0.8        # Solutions: ? and ?\n",
    "batch_size = 32 # Solutions: ? and ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comfortable Option 2: Use the **mouse**\n",
    "Execute the next two cells, use the sliders to adjust the values, then run the rest of the cells.  \n",
    "*Tip:* **Create New View for Output** by right-clicking on the blue bar left of the graphs. Then drag the output window to a comfy place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "style = {'description_width': 'initial'}\n",
    "layout = widgets.Layout(width = '100%')\n",
    "alpha_slider = widgets.FloatLogSlider(value = alpha, min = -4, max = 1, step = 0.001,\n",
    "    description = 'learning rate ' + r'$\\alpha$:', readout_format = '.4f',\n",
    "    style = style, layout = layout)\n",
    "mu_slider = widgets.FloatSlider(value = mu, min = 0, max = 1.0, step = 0.02,\n",
    "    description = 'momentum ' + r'$\\mu$:', readout_format = '.2f',\n",
    "    style = style, layout = layout)\n",
    "batch_slider = widgets.IntSlider(value = batch_size, min = 1, max = m, step = 1,\n",
    "                                 description = 'batch size:', style = style, layout = layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(alpha_slider)\n",
    "display(mu_slider)\n",
    "display(batch_slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = alpha_slider.value\n",
    "mu = mu_slider.value\n",
    "batch_size = batch_slider.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Used values alpha =\", alpha, \", mu =\", mu, \", batch_size =\", batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate = alpha, momentum = mu)\n",
    "# SGD: stochastic gradient descent\n",
    "\n",
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "# This common and preimplemented loss implements the MSE function defined in equation (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "# A tf.data.Dataset has commonly used functions for random sampling, obtaining subsets, transformations\n",
    "# iterating over large numbers of images on disk (using TFRecord).\n",
    "\n",
    "dataset = dataset.shuffle(m).batch(batch_size) # random order and split into batches of size 32\n",
    "\n",
    "# Get a predefined linear model with one single output variable (unit) and one weight per input.\n",
    "theta_init = np.array([2000., 3000.])\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(\n",
    "    units = 1, input_dim = 2,\n",
    "    use_bias = False, # bias equivalent to adding x_0 := 1\n",
    "    kernel_initializer = tf.initializers.Constant(theta_init), # default would be random initialization\n",
    "    dtype = 'float64'))\n",
    "\n",
    "#we compile the model and tell the framework which loss function to use\n",
    "model.compile(optimizer = optimizer, loss = loss_object)\n",
    "\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.theta = []\n",
    "    \n",
    "    def on_epoch_end(self, batch, logs=None):\n",
    "        self.theta.append(tf.squeeze(model.get_weights()[0]))\n",
    "        \n",
    "callback = CustomCallback()\n",
    "\n",
    "start = time.time()\n",
    "#fit the model for 5 epochs (i.e. each datapoint is seen 5 times) \n",
    "#calling this is analogous to our previously implemented low-level training loop\n",
    "#it's just much simpler and in most cases what you want to use to train a model\n",
    "history = model.fit(dataset, epochs=10, callbacks=[callback])\n",
    "end = time.time()\n",
    "print(\"Training took\", end - start, \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_progress(np.stack([theta_init] + callback.theta, -1), history.history[\"loss\"], col='blue');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarks:**\n",
    "  - In above plot of training error, the errors were measured only on a batch and are thus estimates of the training error only.\n",
    "  - When using stochastic gradient descent, typically the parameter trajectory eventually random walks near a local optimum and their latest values may result in a larger loss than some parameter value previously attained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# final loss and prediction computed on all training data\n",
    "pred = model(X_train)\n",
    "pred = tf.reshape(pred, [m])\n",
    "E = loss_object(y_train, pred)\n",
    "print(\"final error = \", E.numpy())\n",
    "print(\"theta:\\n\", model.trainable_variables[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1a664463b03342f4a7986f721e5da2c0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "3679fb2fd42046ad892482a942c15fd2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "100%"
      }
     },
     "56c0ae4232914bb488ef2bcefdcd2abb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntSliderModel",
      "state": {
       "description": "batch size:",
       "layout": "IPY_MODEL_3679fb2fd42046ad892482a942c15fd2",
       "max": 1482,
       "min": 1,
       "style": "IPY_MODEL_1a664463b03342f4a7986f721e5da2c0",
       "value": 32
      }
     },
     "8433fa1b51e649948b4209d13cca1884": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "9828f8e5c1694cec815a4efaeb690e4a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "d9d2ffa0a21d4ae6902fd42a34a88680": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatLogSliderModel",
      "state": {
       "description": "learning rate $\\alpha$:",
       "layout": "IPY_MODEL_3679fb2fd42046ad892482a942c15fd2",
       "max": 1,
       "min": -4,
       "readout_format": ".4f",
       "step": 0.001,
       "style": "IPY_MODEL_8433fa1b51e649948b4209d13cca1884",
       "value": 0.01
      }
     },
     "e027e21313074c078ac16eb21f3586a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatSliderModel",
      "state": {
       "description": "momentum $\\mu$:",
       "layout": "IPY_MODEL_3679fb2fd42046ad892482a942c15fd2",
       "max": 1,
       "step": 0.02,
       "style": "IPY_MODEL_9828f8e5c1694cec815a4efaeb690e4a",
       "value": 0.8
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
