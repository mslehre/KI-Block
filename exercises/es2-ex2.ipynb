{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"text-align:right;\">Mario Stanke, University of Greifswald, Germany</p>\n",
    "\n",
    "# Exercise Set 2 - 2 - Different Activation Functions in a Regression Neural Network\n",
    "\n",
    "This is a copy of notebook ```nn-regr.ipynb``` for solving the following exercises:\n",
    "\n",
    "  0. Run all cells.  \n",
    "   \n",
    "  1. Replace the sigmoid activation function with the **ReLU** function and observe how the shape of the predicted curve changes. *Revert back* afterwards to the *sigmoid* activation function.\n",
    "   \n",
    "  2. **Shift** the simulated data on the **x**-axis by **+50**.  \n",
    "    \n",
    "    2.1. **For the mathematically inclined:** Let $x' := x + 50$. Show or argue that it theoretically does not matter if you shift (use input $x'$) or not (use $x$):  *For any neural net $f(x)$ there is a neural net $f'(x')$ such that $f'(x') = f(x)$*. You may assume that $n=1$.\n",
    "    \n",
    "    2.2. Go to ADD YOUR CODE HERE and follow the instructions. Run all cells. What happens to the goodness of the fit? Do you have an explanation? Tip: Compute the net activations of the first layer manually right before training starts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate an Artificial Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "tf.keras.backend.set_floatx('float64') # floating point precision 64 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    '''A simple one-dimensional function with a few extrema'''\n",
    "    return np.sin(15 * x) + 9 * x - 9 * x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulate $(x, y)$ pairs as artificial data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "m = 100 # number of data points (examples)\n",
    "x = np.linspace(0, 1, m) # equidistant in [0,1] (both boundaries included)\n",
    "y_theoretical = f(x)     # the points on the curve\n",
    "# ADD YOUR CODE HERE to shift x by +50, 1 line\n",
    "\n",
    "# YOUR CODE ENDS\n",
    "sigma = 0.1\n",
    "y = y_theoretical + sigma * np.random.randn(m) # observations have noise (normal with mean 0 and variance sigma^2 = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curve():\n",
    "    fig, ax = plt.subplots(figsize = (10, 6))\n",
    "    ax.plot(x, y, 'ko', fillstyle = 'none', markersize = 4, label = \"sampled, noisy data\") # ko: blac(k) circle\n",
    "    ax.plot(x, y_theoretical, 'r', label = \"true unknown function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    return ax\n",
    "\n",
    "ax = plot_curve()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Neural Network Model  \n",
    "We will use [```tf.keras```](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras), a high-level approach to neural network design and learning. The NN shall have 2 hidden layers with 8 neurons each. As activation functions we chose the logistic sigmoid function $\\sigma$.\n",
    "\n",
    "<img src=\"nn8-8.png\" alt=\"network architecture\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(13032021) # so we all get the same pseudorandom results\n",
    "\n",
    "# a neural network is a stack of layers, in keras called \"Sequential\" model\n",
    "model = tf.keras.models.Sequential() # so far the stack is empty, 0 layers\n",
    "\n",
    "# add neural network layers one by one\n",
    "# Dense is a fully connected layer, parametrized by a matrix of shape (input_units, output_units).\n",
    "# The first number given in Dense is the number of output units, the number of input units is implicit.\n",
    "# By default use_bias=True which adds output_units parameters for each layer\n",
    "# REPLACE THE ACTIVATION FUNCTION HERE with tf.nn.relu\n",
    "model.add(Dense(8, activation = tf.nn.sigmoid, input_dim = 1)) # input dimension only required for first layer\n",
    "model.add(Dense(8, activation = tf.nn.sigmoid))\n",
    "# YOUR CHANGES END HERE\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.summary()     # summarizes layers and parameters\n",
    "Theta = model.get_weights() # a look under the 'hood' for teaching purposes\n",
    "print(Theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation, why the training with the x-shift by +50 fails\n",
    "To see this we will manually compute the **activations of the first layer right after the random initialization** of the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta0 = Theta[0] # weights of the first layer, shape =(1,8)\n",
    "bias0 = Theta[1] # biases of the first layer, shape = (8,) \n",
    "\n",
    "# compute net activations of first layer\n",
    "z1 = np.matmul(x.reshape([-1,1]), Theta0) + bias0\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The logistic sigmoid function\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    \"\"\" The derivative of sigmoid\"\"\"\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "\n",
    "\n",
    "print(\"z1\\n\", z1[0:5].round(3)) # print the net activations of the first layer for the first 5 examples\n",
    "print(\"\\na1\\n\", sigmoid(z1[0:5])) # print the net activations of the first layer for the first 10 examples\n",
    "print(\"\\nderivatives used in backprop:\\n\", sigmoid_deriv(z1[0:5])) # print the net activations of the first layer for the first 10 examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** Almost all net activations of the first layer have a large absolute value. For large absolut value the logistic sigmoid function has a derivative very close to 0:\n",
    "\n",
    "<img src=\"activ-sigma.png\" alt=\"sigmoid function\" width=\"300\"/>\n",
    "\n",
    "Therefore, any small change to any weight of an edge of the first layer that goes into such a unit is very close to 0 or even vanishes numerically.\n",
    "\n",
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss, optimization algorithm and prepare the model for gradient computation \n",
    "opti = tf.keras.optimizers.Adam(learning_rate = 0.05) # Adam is a popular method for stochastic gradient descent\n",
    "model.compile(optimizer = opti, loss = 'mse') # mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the parameters (learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute the actual training \n",
    "history = model.fit(x, y, epochs=3000, verbose=0) # takes ~20s\n",
    "# verbose = 1,2 gives more output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the learning progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log10(history.history[\"loss\"])); # one mse value per epoch\n",
    "plt.title(\"training loss\")\n",
    "plt.xlabel(\"training epoch\")\n",
    "plt.ylabel(r\"$\\log_{10}$ mean squared error\");\n",
    "plt.axhline(y = np.log10(sigma**2), color = \"red\", linestyle = \":\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that with our choice for the measurement noise the theoretical optimum for the mean squared error *on new data* is $10^{-2}$ (red dotted line).\n",
    "### Use the model for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = x.reshape((-1, 1)) # make x a matrix (with 1 column) as expected by predict\n",
    "y_pred = model.predict(x_input)\n",
    "\n",
    "ax = plot_curve()\n",
    "ax.plot(x, y_pred, 'b', label = \"neural network estimates\") # (b)lue\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
