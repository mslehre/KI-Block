{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c5b0b3b-06c4-496a-b510-deb97ac3e4a0",
   "metadata": {},
   "source": [
    "# Transformers: Fine-Tuning\n",
    "\n",
    "In this notebook, you will load a language model that was pre-trained on a general text corpus and fine tune it on a more specific corpus. This is a very common practice in NLP, where the costly workload of training a large language model from scratch is outsourced. The user only loads a pre-trained model and fine-tunes it for a few epochs on text data of interest.\n",
    "\n",
    "## Task\n",
    "\n",
    "Follow the notebook, understand what's going on.\n",
    "\n",
    "This excercise is based on https://huggingface.co/course/chapter7/3?fw=tf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d121d4cc-d0cc-463e-a9eb-c468e9745ca8",
   "metadata": {},
   "source": [
    "## 1. Load a pretrained language model\n",
    "\n",
    "Load the pretrained checkpoint \"distilbert\" (https://huggingface.co/distilbert-base-uncased) which is a \"distilled\" (smaller and faster) version of the BERT base model. It was trained on the datasets [wikipedia](https://huggingface.co/datasets/wikipedia) and [bookcorpus](https://huggingface.co/datasets/bookcorpus) and is \"teached\" (details omitted) by the original BERT model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6c4e568-3713-445d-9203-df759f12ebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForMaskedLM, AutoTokenizer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from functools import partial \n",
    "import TransformerUtility as tut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ce3023e-fac6-43ee-98c2-aca77333a1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForMaskedLM: ['activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#browse https://huggingface.co/models to find more models\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba77bb46-0023-476d-b4d3-06857db2ad93",
   "metadata": {},
   "source": [
    "If the previous cell prints a warning, this is expected and can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "200a6d94-b804-494b-8f9c-561c1c7db720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The language model distilbert-base-uncased has 67M parameters.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"The language model {model_checkpoint} has {round(model.count_params()/1e6)}M parameters.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f9d38-e08f-454a-aa6d-5a4513a4caa7",
   "metadata": {},
   "source": [
    "## 2. Let the LM autocomplete a masked sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7493769-e79a-4a4c-a357-fdb4ffa11090",
   "metadata": {},
   "source": [
    "To convert a query from a string into a representation that can be consumed by a language model, we have to tokenize it. The tokenizer can also be used to convert a token representation of a text back to strings.\n",
    "\n",
    "Usually, each model checkpoint at huggingface.co comes with its own tokenizer. We just have to load it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fedc58f-4e1e-4a68-bea2-f633814f9d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211842f6-fec9-4d27-94b7-af18ff19473d",
   "metadata": {},
   "source": [
    "Running the model works like this:\n",
    "1. Tokenize a string (that can contain [MASK] to indicate masked words). This generates a list of tensors that will be fed to the model.\n",
    "2. Pass the generated inputs to the model and receive logits. The last axes of the output tensor is usually very large, because there are many different tokens possible. The model generates predictions for all positions, not just the masked ones. \n",
    "3. Decide for tokens with high probability based on the logits and use the tokenizer to decode back to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15e7d202-8616-407b-92fd-d141dbd2598e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [\"This is a great [MASK].\",\n",
    "         \"This [MASK] is a disappointment.\", \n",
    "         \"I like to [MASK] [MASK] with my friends.\",\n",
    "         \"I would like to [MASK] this.\"]\n",
    "lens = [len(s.split()) for s in query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7425d01a-a36e-4e2a-acb9-467f433d37ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[  101,  2023,  2003,  1037,  2307,   103,  1012,   102,     0,\n",
       "            0,     0],\n",
       "       [  101,  2023,   103,  2003,  1037, 10520,  1012,   102,     0,\n",
       "            0,     0],\n",
       "       [  101,  1045,  2066,  2000,   103,   103,  2007,  2026,  2814,\n",
       "         1012,   102],\n",
       "       [  101,  1045,  2052,  2066,  2000,   103,  2023,  1012,   102,\n",
       "            0,     0]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.\n",
    "inputs = tokenizer(query, return_tensors=\"np\", padding=True, truncation=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d49fdd1-8e22-4536-8f63-662946543ec1",
   "metadata": {},
   "source": [
    "We can see that strings have been converted to token ids. The meaning of these ids is encapsuled in the tokenizer. We also have a padding mask that indicates with a 0, if a position is not part of an actual input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "153e2a61-9856-4a0b-bf96-90933da89bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30522,), dtype=float32, numpy=\n",
       "array([-5.5882792, -5.5876465, -5.5964637, ..., -4.9451265, -4.817601 ,\n",
       "       -2.9901636], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.\n",
    "token_logits = model(**inputs).logits\n",
    "token_logits[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0c9d20-a4a6-4402-8993-37540539b311",
   "metadata": {},
   "source": [
    "Remember that the model outputs logits, not probabilities. If you want to see probabilities, use `tf.nn.softmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "280b4080-4198-433c-b926-b9f2eb22bba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a great deal\n",
      "this book is a disappointment\n",
      "i like to spend fun with my friends\n",
      "i would like to discuss this\n",
      "\n",
      "this is a great success\n",
      "this article is a disappointment\n",
      "i like to enjoy friends with my friends\n",
      "i would like to do this\n",
      "\n",
      "this is a great adventure\n",
      "this movie is a disappointment\n",
      "i like to be out with my friends\n",
      "i would like to hear this\n",
      "\n",
      "this is a great idea\n",
      "this episode is a disappointment\n",
      "i like to play chat with my friends\n",
      "i would like to repeat this\n",
      "\n",
      "this is a great feat\n",
      "this project is a disappointment\n",
      "i like to share along with my friends\n",
      "i would like to know this\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3.\n",
    "#find the [MASK] positions and just take the most likely predicted token\n",
    "def decode(logits, best_k = 5):\n",
    "    mask_tokens = inputs[\"input_ids\"] == tokenizer.mask_token_id\n",
    "    mask_logits = logits[mask_tokens]\n",
    "    #get the indices of the top logits\n",
    "    top_tokens = np.argsort(-mask_logits, -1)[:,:best_k]\n",
    "    for k in range(best_k):\n",
    "        autocompleted_query = np.array(inputs.input_ids) #make a copy\n",
    "        autocompleted_query[mask_tokens] = top_tokens[:,k]\n",
    "        #remove start- and end-tokens and decode\n",
    "        for i, l in enumerate(lens):\n",
    "            print(tokenizer.decode(autocompleted_query[i,1:1+l]))\n",
    "        print(\"\")\n",
    "        \n",
    "decode(token_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5f72de-4397-4daf-8a66-c07ff4eeb6d4",
   "metadata": {},
   "source": [
    "Note that if we have multiple [MASK] tokens in one query, its naive to take the most likely predictions independently. Often, the resulting sentences do not make much sense. A better way would be to replace one [MASK] at a time and rerun the model afterwards to select the remaining [MASK]s (left open for exercise at home)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121ea199-b3a9-4f59-bd60-b67185261cfa",
   "metadata": {},
   "source": [
    "## 3. Prepare the fine-tuning dataset\n",
    "\n",
    "We will use [imdb](https://huggingface.co/datasets/imdb) for fine-tuning which contains pairs of movie reviews and ratings (neg./pos.). We will ignore the ratings and just learn on the text.\n",
    "\n",
    "You will learn that fine-tuning (as opposed to training from scratch) is fast. The resulting model retains the general language understanding capabilites of the pre-trained model, but the outputs will (hopefully) be more specific, reflecting the domain we are interested in (in this case movie reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7a48a0c-c75d-43b2-aacf-2c55307b5c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/jovyan/.local/lib/python3.9/site-packages (2.10.1)\n",
      "Requirement already satisfied: aiohttp in /home/jovyan/.local/lib/python3.9/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/jovyan/.local/lib/python3.9/site-packages (from datasets) (9.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jovyan/.local/lib/python3.9/site-packages (from datasets) (1.19.5)\n",
      "Requirement already satisfied: multiprocess in /home/jovyan/.local/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/jovyan/.local/lib/python3.9/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/jovyan/.local/lib/python3.9/site-packages (from datasets) (2023.1.0)\n",
      "Requirement already satisfied: responses<0.19 in /home/jovyan/.local/lib/python3.9/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets) (1.2.4)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/jovyan/.local/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: packaging in /home/jovyan/.local/lib/python3.9/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: xxhash in /home/jovyan/.local/lib/python3.9/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /home/jovyan/.local/lib/python3.9/site-packages (from datasets) (0.11.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jovyan/.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jovyan/.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/jovyan/.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/jovyan/.local/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/jovyan/.local/lib/python3.9/site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jovyan/.local/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: filelock in /home/jovyan/.local/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "#huggingface.co provides an API to access their datasets\n",
    "#we simply have to install their package called \"datasets\"\n",
    "#feel free to roam huggingface.co for other interesting datasets than those used here\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8c2bc1d-4446-4337-aac4-9f1209c95918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/jovyan/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "758ee2e5650d49f9968e431fb212104a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#browse https://huggingface.co/datasets to find more datasets\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4813f2-9752-41be-bd9a-a00a81f64cd4",
   "metadata": {},
   "source": [
    "We can see that the dataset already comes preconfigured for training and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38311309-1aab-4ea8-8dd2-560461c14f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(imdb_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd7648-d379-4355-b13e-ea9a35f950ad",
   "metadata": {},
   "source": [
    "They use a custom class for their datasets (which is ok, because they also provide a pipeline for training). However, if we wish, we could also covert to a pandas dataset. This would be a more general format that is compatible with a lot of other APIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed590057-d739-4fff-84b8-0eddbc5262b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>A hit at the time but now better categorised a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>I love this movie like no other. Another time ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>This film and it's sequel Barry Mckenzie holds...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>'The Adventures Of Barry McKenzie' started lif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>The story centers around Barry McKenzie who mu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
       "1      \"I Am Curious: Yellow\" is a risible and preten...      0\n",
       "2      If only to avoid making this type of film in t...      0\n",
       "3      This film was probably inspired by Godard's Ma...      0\n",
       "4      Oh, brother...after hearing about this ridicul...      0\n",
       "...                                                  ...    ...\n",
       "24995  A hit at the time but now better categorised a...      1\n",
       "24996  I love this movie like no other. Another time ...      1\n",
       "24997  This film and it's sequel Barry Mckenzie holds...      1\n",
       "24998  'The Adventures Of Barry McKenzie' started lif...      1\n",
       "24999  The story centers around Barry McKenzie who mu...      1\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset[\"train\"].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41950b14-8191-449d-9ae6-05a537763a24",
   "metadata": {},
   "source": [
    "Lets look at a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6004ca72-7f7b-4a04-8ee5-6317deb51e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/jovyan/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-9c48ce5d173413c7.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> Review: There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier's plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it's the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...'\n",
      "'>>> Label: 1'\n",
      "\n",
      "'>>> Review: This movie is a great. The plot is very true to the book which is a classic written by Mark Twain. The movie starts of with a scene where Hank sings a song with a bunch of kids called \"when you stub your toe on the moon\" It reminds me of Sinatra's song High Hopes, it is fun and inspirational. The Music is great throughout and my favorite song is sung by the King, Hank (bing Crosby) and Sir \"Saggy\" Sagamore. OVerall a great family movie or even a great Date movie. This is a movie you can watch over and over again. The princess played by Rhonda Fleming is gorgeous. I love this movie!! If you liked Danny Kaye in the Court Jester then you will definitely like this movie.'\n",
      "'>>> Label: 1'\n",
      "\n",
      "'>>> Review: George P. Cosmatos' \"Rambo: First Blood Part II\" is pure wish-fulfillment. The United States clearly didn't win the war in Vietnam. They caused damage to this country beyond the imaginable and this movie continues the fairy story of the oh-so innocent soldiers. The only bad guys were the leaders of the nation, who made this war happen. The character of Rambo is perfect to notice this. He is extremely patriotic, bemoans that US-Americans didn't appreciate and celebrate the achievements of the single soldier, but has nothing but distrust for leading officers and politicians. Like every film that defends the war (e.g. \"We Were Soldiers\") also this one avoids the need to give a comprehensible reason for the engagement in South Asia. And for that matter also the reason for every single US-American soldier that was there. Instead, Rambo gets to take revenge for the wounds of a whole nation. It would have been better to work on how to deal with the memories, rather than suppressing them. \"Do we get to win this time?\" Yes, you do.'\n",
      "'>>> Label: 0'\n"
     ]
    }
   ],
   "source": [
    "sample = imdb_dataset[\"train\"].shuffle(seed=42).select([0,1,2])\n",
    "\n",
    "for row in sample:\n",
    "    print(f\"\\n'>>> Review: {row['text']}'\")\n",
    "    print(f\"'>>> Label: {row['label']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6046de-4e0b-48d9-83c3-84673772b28a",
   "metadata": {},
   "source": [
    "The preprocessing works as follows:\n",
    "1. Tokenize all reviews. We have already learned how to do this before.\n",
    "2. Concatenate everything and split it into equal sized chunks. I.e. we glue all review to a single large chunk of text and \n",
    "3. Downsample the dataset to speed up training.\n",
    "\n",
    "To keep this notebook simple, the details of these steps are defined in the TransformerUtility module. Feel free to take a look at the code there. We can use the datasets `map` function, to apply one of our own functions to all examples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32f2b964-ed6b-416c-863d-5493cc636f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-90cc70ec80c747e4.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-f582002f9c3c1ebf.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-0e2554afe38ff41c.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.\n",
    "tokenized_datasets = imdb_dataset.map(\n",
    "    partial(tut.tokenize_function, tokenizer), \n",
    "    batched=True, remove_columns=[\"text\", \"label\"])\n",
    "# 2.\n",
    "lm_datasets = tokenized_datasets.map(tut.group_texts, batched=True)\n",
    "# 3. \n",
    "downsampled_train = 10_000\n",
    "downsampled_test = int(0.1 * downsampled_train)\n",
    "\n",
    "downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n",
    "    train_size=downsampled_train, test_size=downsampled_test, seed=42)\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9559afb6-ce1e-4fd1-8006-5b80fe19bf67",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f292f882-83dd-4340-9fb6-7bd9e7ebd69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0b8ca55-f91a-4feb-941a-99472dede3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "tf_train_dataset = model.prepare_tf_dataset(\n",
    "    downsampled_dataset[\"train\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "tf_eval_dataset = model.prepare_tf_dataset(\n",
    "    downsampled_dataset[\"test\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06c86ed9-7629-449d-baa3-8861d989a81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import create_optimizer\n",
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "import tensorflow as tf\n",
    "\n",
    "num_train_steps = len(tf_train_dataset)\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=1e-4,\n",
    "    num_warmup_steps=100,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    ")\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b0c5cb8-5060-4177-b960-4d72692913a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function input_processing at 0x7f23aed601f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: '<' not supported between instances of 'Literal' and 'str'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function input_processing at 0x7f23aed601f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: '<' not supported between instances of 'Literal' and 'str'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "32/32 [==============================] - 3s 44ms/step - loss: 3.1672\n",
      "Perplexity: 23.74\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_loss = model.evaluate(tf_eval_dataset)\n",
    "print(f\"Perplexity: {math.exp(eval_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbc8f5c-c6a0-4059-a925-cf1bc913ef09",
   "metadata": {},
   "source": [
    "A common metric to evaluate NLP models is perplexity. Intuitively it measures how surprised the model is by the ground truth. Lower perplexity indicates a better model. It's out of scope here to discuss the mathematical details of perplexity.\n",
    "\n",
    "Note that we should not use accuracy. The model might choose a synonym token all the time and achieve zero accuracy despite being actually a good fit. Perplexity is computed directly from the cross-entropy loss, has an intuitive interpretation and takes the predictions for all tokens, not just the single \"correct\" one, into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1aceeb5e-a2ee-46c9-986c-7679ea83fef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "312/312 [==============================] - 39s 119ms/step - loss: 2.6613 - val_loss: 2.4620\n",
      "Epoch 2/3\n",
      "312/312 [==============================] - 37s 120ms/step - loss: 2.5186 - val_loss: 2.4427\n",
      "Epoch 3/3\n",
      "312/312 [==============================] - 38s 121ms/step - loss: 2.5131 - val_loss: 2.4023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f250947e6d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67d6b309-1b61-4b39-ac8b-f9c94dc586c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 44ms/step - loss: 2.4709\n",
      "Perplexity: 11.83\n"
     ]
    }
   ],
   "source": [
    "eval_loss = model.evaluate(tf_eval_dataset, )\n",
    "print(f\"Perplexity: {math.exp(eval_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9e16e8-a1e7-4245-b94a-d746ade2bc4d",
   "metadata": {},
   "source": [
    "## 5. Repeat the example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dae71c-68f1-489a-b535-5ed5be6451fd",
   "metadata": {},
   "source": [
    "If done correctly, we should see that the outputs of the fine-tuned model reflect the imdb corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5632939-3284-40c4-924b-66b68a9341b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a great film\n",
      "this movie is a disappointment\n",
      "i like to spend friends with my friends\n",
      "i would like to see this\n",
      "\n",
      "this is a great movie\n",
      "this film is a disappointment\n",
      "i like to be up with my friends\n",
      "i would like to know this\n",
      "\n",
      "this is a great idea\n",
      "this one is a disappointment\n",
      "i like to go fun with my friends\n",
      "i would like to enjoy this\n",
      "\n",
      "this is a great story\n",
      "this book is a disappointment\n",
      "i like to have time with my friends\n",
      "i would like to hear this\n",
      "\n",
      "this is a great adventure\n",
      "this episode is a disappointment\n",
      "i like to share out with my friends\n",
      "i would like to do this\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finetuned_token_logits = model(**inputs).logits\n",
    "decode(finetuned_token_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08f1e4aa-9bfa-4e36-999e-3076bbc7ba40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a great deal\n",
      "this book is a disappointment\n",
      "i like to spend fun with my friends\n",
      "i would like to discuss this\n",
      "\n",
      "this is a great success\n",
      "this article is a disappointment\n",
      "i like to enjoy friends with my friends\n",
      "i would like to do this\n",
      "\n",
      "this is a great adventure\n",
      "this movie is a disappointment\n",
      "i like to be out with my friends\n",
      "i would like to hear this\n",
      "\n",
      "this is a great idea\n",
      "this episode is a disappointment\n",
      "i like to play chat with my friends\n",
      "i would like to repeat this\n",
      "\n",
      "this is a great feat\n",
      "this project is a disappointment\n",
      "i like to share along with my friends\n",
      "i would like to know this\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#output before fine-tuning:\n",
    "decode(token_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3950814f-5107-4718-92ed-6d5380aeceb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
