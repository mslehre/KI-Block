{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee8ab9a2-c05a-42dd-9584-efeb8abeaf4e",
   "metadata": {},
   "source": [
    "# 2 Classification of snoRNAs with Convolutional Neural Networks (CNN) with the Primary Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d30328-6e25-4115-ae03-5642efc6a2de",
   "metadata": {},
   "source": [
    "## The Basic CNN architecture for our first predictions\n",
    "\n",
    "We want to use the package keras from Tensorflow for our classification. \n",
    "\n",
    "Keras provides a very simple type of model class, the [Sequential Model](https://keras.io/api/models/sequential/). To create a classification model, you simply have to create an instance of the sequential class\n",
    "\n",
    "Then, you can add the desired layers using the `.add()` method.\n",
    "\n",
    "### Our basic architecture\n",
    "\n",
    "The basic classifier we want to use has the following architecture:\n",
    "\n",
    "One `Conv1D()` layer ([Doc](https://keras.io/api/layers/convolution_layers/convolution1d/)). This performs the convolutions for use. We will use 32 filters (sometimes called kernels) of size 7 (scanning the sequence in windows of 7 nucelotides). The activation function will be [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)), and we will perform [\"zero-padding\"](https://deepai.org/machine-learning-glossary-and-terms/padding) (\"same\"; evenly adding zeros to the left and right if we have in the size of seven not 7 nucelotides). Lastly, we need to specify the input dimensions. In our dataset we define the input as (660, 1) meaning a size of 600 (representing the amount of nucleotides) for each sequence and one dimension.\n",
    "\n",
    "Next is the [Max Pooling](https://keras.io/api/layers/pooling_layers/max_pooling1d/) step, with a poolsize of 4.\n",
    "\n",
    "After that, we will add a [Dropout](https://keras.io/api/layers/regularization_layers/dropout/) layer. Dropout randomly removes a specified fraction of the neural connections (in our case 50%) to avoid overfitting. Dropout is one of many methods of [Regularization](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a), which aim to stop the method from simply memorizing the training data and thus struggling with test data.\n",
    "\n",
    "Next is the [flattening](https://keras.io/api/layers/reshaping_layers/flatten/) step to combine the 32 filters into one again.\n",
    "\n",
    "Lastly, we will output the label using a [Dense](https://keras.io/api/layers/core_layers/dense/) layer. This layer has to have one node for each label. We will use the [Softmax](https://en.wikipedia.org/wiki/Softmax_function) function, which outputs probabilities for each label between 0 and 1. \n",
    "\n",
    "Our model will train using the [Adam](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/) optimizer, our loss will be determined through [Categorical Crossentropy](https://gombru.github.io/2018/05/23/cross_entropy_loss/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbae313e-4fd2-4e7c-b947-bbfa7d6c0737",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "def create_cnn_model_1():\n",
    "    \n",
    "    # create the model\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Add a convolutional layer\n",
    "    model.add(keras.layers.Conv1D(32, 7, activation='relu', padding='same', input_shape=(600, 1)))\n",
    "    \n",
    "    # Pool the sequences to the maximum value\n",
    "    model.add(keras.layers.MaxPooling1D(4))\n",
    "    \n",
    "    # Add Dropout to avoid overfitting\n",
    "    model.add(keras.layers.Dropout(rate = 0.5))\n",
    "    \n",
    "    # \"Flatten\" the output (combining all the values from the previous layer into a vector)\n",
    "    model.add(keras.layers.Flatten())\n",
    "\n",
    "    # Output the prediction\n",
    "    model.add(keras.layers.Dense(3, activation=\"softmax\")) \n",
    "\n",
    "    # Create the optimizer that will do the learning for us\n",
    "    Adam = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    \n",
    "    # Compile the model and set the loss function\n",
    "    model.compile(optimizer=Adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac86f382-7823-4bfa-94a2-9c3599280560",
   "metadata": {},
   "source": [
    "## Reading and pre-process the sequences as input file for the CNN\n",
    "\n",
    "First we need to read in the data set and perform some pre-processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7037051a-5815-4e83-95bc-2b4f72c322da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "full_df = pd.read_csv(\"full_df.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cbb07a-0ca2-40d9-9ba1-4de6f67d3a51",
   "metadata": {},
   "source": [
    "\n",
    "### Preprocessing\n",
    "\n",
    "We cannot simply input the sequences directly like images but have to first define a common size in our case length of all the sequences. As sequences are not of the same length we have to artificially increase the shorter sequences to a common length or remove nucleotides until we have a minimal common length which would reduce the amount of information. In our example here we know that no seqeunce is longer than 600 nucleotides (nt). For this reason we choose 600 as our common length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648fd11c-3fad-44c9-af4b-3c0df14684d4",
   "metadata": {},
   "source": [
    "### Padding the sequences\n",
    "\n",
    "The input of convolutional neural networks has to be a fixed length, but our sequences have very heterogene lengths. We will fix this, by \"padding\" the sequences to a length of 600 nucleotides using a filler character. THis padding is different from the padding command of the CNN [Conv1D] layer. The \"padding\" here is in the beginning to make the input equal and the padding in the convolutional layer is coping with uneven filter sizes during the window shifting.\n",
    "\n",
    "The Python methods `.rjust()` and `.ljust()` take our nucleotide sequence as string and append a character to the left of it (`rjust`) until a specific length is reached and vice versa for `ljust`.  \n",
    "\n",
    "We will create a new column in our dataframe that pads the sequences on the left using `full_df.sequence.str.rjust(600, \"_\")`. This will append the \"_\" character on the left of the sequence until the sequence is 600 nt long. You can use any character you like with the restriction not to choose any from your alphabet in the sequences. Note, that you will have to put `.str.` between the column and rjust, because rjust will not work on a series. `.str.` treats the entries of the series individually. You can read more about rjust and ljust [here](https://www.tutorialsteacher.com/python/string-rjust). We will start by padding our sequences on the left using `rjust()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7eada1-186b-48c1-91ba-3cf052855cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[\"seq_pad_left\"] = full_df[\"sequence\"].str.rjust(600, \"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0e0a42-157c-46c8-83ad-abf890635473",
   "metadata": {},
   "source": [
    "What do the sequences look like now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e307c446-e5dc-46fd-bf9d-53b321581f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_df[\"seq_pad_left\"][0])\n",
    "print(f\"\\nSequence length: {len(full_df['seq_pad_left'][0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f36df8-33b7-45c8-9bdf-7ca9ffc11612",
   "metadata": {},
   "source": [
    "But after the \"padding\" we now have 600 nucleotides and a character of your choice. But Keras models require the input to be as numerical values or tuples of numerical values so we have to process our sequences from strings of characters to integers or floats or tuples of integers or floats. \n",
    "\n",
    "We will have to transform our sequences into lists of numbers. Python provides the `ord()` [function](https://www.w3schools.com/python/ref_func_ord.asp), which encodes characters into their ASCII value. So by this we can encode the sequence characters as numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be65971-7588-4592-8b0c-e90e2485b76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[\"ord_encoded_left\"] = full_df[\"seq_pad_left\"].map(lambda x: [ord(char) for char in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddc0bb2-e41f-4027-8619-0f2390ea8c8e",
   "metadata": {},
   "source": [
    "This will look confusing at first, but I will try to break it down into smaller pieces:\n",
    "\n",
    "`full_df[\"seq_pad_left\"]` takes the desired column\n",
    "\n",
    "`.map()` applies a function to every entry in the column to create a new list\n",
    "\n",
    "`lambda x: ` is a neat python trick to create temporary functions. Basically, you tell python, that you want to everything after the \":\" done to x. You can read more on lambda functions [here](https://www.w3schools.com/python/python_lambda.asp)\n",
    "\n",
    "`[ord(char) for char in x]` is what we want to do to \"x\". The brackets `[ ... ]` tell python, that you want to create a new list. `ord(char) for char in x` means, that we want to iterate through every character in our string x and transform it into its ordinal encoding. `char` in this case is just a variable name. You can read more on list comprehension [here](https://www.w3schools.com/python/python_lists_comprehension.asp)\n",
    "\n",
    "So, what we end up with is a new column, which for each sequence will have a list of integers (numbers) corresponding to the encoding of the nucleotides.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b853fd05-9649-46d0-bd5b-ff5b2ac17d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_df[\"ord_encoded_left\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499837f5-b952-4694-bd53-493e607fac47",
   "metadata": {},
   "source": [
    "`ord()` encoded \"A\" as 65, \"C\" as 67, \"G\" as 71 and \"T\" as 84. The padding character \"\\_\" is encoded as 95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724ad96a-be8e-4dfe-a6b6-fb4d01c33e8d",
   "metadata": {},
   "source": [
    "## Preparing our datasets to perform the first model training\n",
    "\n",
    "Just like with the Naive Bayes classifiers, we need to split the data into train and test. If you want to use the same split from the first notebook, make sure you set the random state to the same number as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0363ec80-16b0-4a6f-abaf-7e018faa2ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(full_df[[\"ord_encoded_left\"]], full_df[\"type\"], test_size=0.15, random_state=XXXX, stratify=full_df[\"type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9d82a5-f675-4366-9569-c6ab8ace5ff1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Our numerically encoded sequences will need to be transformed into a numpy array as well, otherwise Keras will not be able to use them. Do that using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c82944-5adf-4ad1-918d-a8c73ebeb1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Transform the training input into an array and reshape it so it fits the model\n",
    "X_train_array_left = np.array(X_train[\"ord_encoded_left\"].to_list()).reshape(4878, 600, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d12fe15-7ae9-48be-8da2-65679a263fa7",
   "metadata": {},
   "source": [
    "This will look confusing at first. This is because we are handling different data types that need intermediate steps to be transformed into each other. Let's break it down in the order of operations:\n",
    "\n",
    "`X_train[\"ord_encoded_left\"]` accesses the desired column we want for our model input. This column is of the type \"series\", whose entries are lists of integers.\n",
    "\n",
    "`.to_list()` turns the series into a list. A series and a list are essentially the same, but internally they are handled slightly differently. We need this intermediate step because...\n",
    "\n",
    "`np.array()` takes a list of lists and transforms them into a matrix (an array). This is almost ready for our machine learning model, we only need to \n",
    "\n",
    "`.reshape(4878, 600, -1)` the data. This is because Keras expects the data to be of a certain shape. 4878 is the number of samples in the training set, 600 is the length of each row. \n",
    "\n",
    "We need to do the same for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09c59b0-f63e-4542-b65c-396a806e5af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_array_left = np.array(X_test[\"ord_encoded_left\"].to_list()).reshape(861, 600, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b517eea-2ab6-4a03-92be-2ad7b12b3711",
   "metadata": {},
   "source": [
    "We are almost ready to perform our first CNN classification. The only thing left to do, is encoding the output using [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html). Since our model outputs a probability distribution between 0 and 1 from the softmax function, we need to encode each class as a tuple of values like `[1, 0, 0]` (100% C/D-Box). The one-hot encoder will encode C/D-Box as `[1, 0, 0]`, H/ACA-Box as `[0, 1, 0]` and scaRNA as `[0, 0, 1]`. If the model outputs `[0.60, 0.25, 0.15]` this will mean the model thinks the sequence is 60% C/D-Box, 25% H/ACA-Box, 15% scaRNA.\n",
    "\n",
    "Note, that for fitting and transforming the list of labels, you will need to turn them into a numpy array using `np.array()` and reshape them using `.reshape(-1, 1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728917e6-aad2-4598-8bfc-ed85d855f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False)  # Create an instance of the OneHotEncoder class. \n",
    "# sparse=True would be useful for instances with a lot of different labels, but for this case we don't need it\n",
    "\n",
    "ohe.fit(np.array(Y_train).reshape(-1, 1))  # Fit the encoder to our data\n",
    "\n",
    "Y_train_ohe = ohe.transform(np.array(Y_train).reshape(-1, 1))  # Transform the training labels\n",
    "\n",
    "Y_test_ohe = ohe.transform(np.array(Y_test).reshape(-1, 1))  # Transform the test labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512f4f15-a9e9-44ea-81fc-2e8327ce6f47",
   "metadata": {},
   "source": [
    "## Train the CNN model\n",
    "\n",
    "Now we are finally ready to train our first CNN classifier. We will train the model for 20 epochs with a [batch size](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/) of 40. So every 40 sequences the model will get an update of the internal parameter. `verbose` indicates, how much information will be output during training (If you want a progress bar for each epoch, you can set it to 1). The `validation_split` is the fraction of training data, that is reserved for validation during training. After each epoch, the model is tested on the validation data, such that you know how well the model performs on unseen data. Note, that this is not the test set, which will come into play later. You can read about the difference between test and validation data [here](https://machinelearningmastery.com/difference-test-validation-datasets/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f854c75-2443-49ac-94ea-0a13ff2f64c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the above model\n",
    "model = create_cnn_model_1()\n",
    "\n",
    "# Fitting the model with the train data. You can also save the history to look at the training in more detail\n",
    "history_left = model.fit(X_train_array_left, Y_train_ohe, epochs=20, batch_size=40, verbose=2, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f62721-0481-4da2-ac6e-df3d946064d9",
   "metadata": {},
   "source": [
    "You should now have an output for each epoch that looks roughly like this: \n",
    "\n",
    "`Epoch 9/10`\n",
    "\n",
    "`104/104 - 1s - loss: 0.3776 - accuracy: 0.8570 - val_loss: 0.4293 - val_accuracy: 0.8429 - 633ms/epoch - 6ms/step`\n",
    "\n",
    "`104/104` is the progress. Essentially, this iterates through every training batch.\n",
    "\n",
    "`1s` is the time passed for each epoch. This will increase with model complexity.\n",
    "\n",
    "`loss: ` is the training [loss](https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/), essentially, how far off are the predictions on average?\n",
    "\n",
    "`accuracy: ` is the training accuracy. How good is the prediction on the training set.\n",
    "\n",
    "`val_loss` and `val_accuracy` are the same, but on the validation data. These are the measures we want to look at, since they indicate how well our model can predict unseen data. \n",
    "\n",
    "Usually, you would want to train until you are satisfied with the val_loss and val_accuracy. Due to time constraints we will limit training to 20 epochs for now. At some point, all models will plateau and it will no longer learn any meaningful information from the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c401ae70-e5e4-4b3e-9100-29f4949844ee",
   "metadata": {},
   "source": [
    "We now have our first trained model with internal validation but now need an unseen dataset to verify our learning progress. Therefore we will use our testing data, we excluded so far. \n",
    "\n",
    "We can now use `.predict()` for the trained model to find on our new dataset the predicted ncRNA class for each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e442148-26a3-4593-9a01-298850c0a52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_left = model.predict(X_test_array_left)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ec62da-123a-4aca-98df-b588995fc2ee",
   "metadata": {},
   "source": [
    "Let's look at our prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbcac22-725a-4ff5-b4cb-c4540aeb43aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction_left)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852dd182-551d-4633-8e0c-c044beff328b",
   "metadata": {},
   "source": [
    "The model outputs  at stage the encoded probabilities for each class. If we want to look at the classification report, we will have to turn the prediction back into the labels.\n",
    "\n",
    "Luckily, our OneHotEncoder has a method for inverse transformation, that will turn our probabilities into the label.\n",
    "\n",
    "Use `.inverse_transform()` and then look at the classification report. We will also save some of the variables in the classification report, such that we can plot them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc92eb4-03a2-4e00-8e18-cb587421dc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Transform predictions back into labels\n",
    "pred_labels_left = ohe.inverse_transform(prediction_left)\n",
    "\n",
    "# Output the classification report\n",
    "print(classification_report(Y_test, pred_labels_left))  \n",
    "\n",
    "# Save the scores for plotting later\n",
    "f1_left = f1_score(Y_test, pred_labels_left, average=\"weighted\")\n",
    "prec_left = precision_score(Y_test, pred_labels_left, average=\"weighted\")\n",
    "rec_left = recall_score(Y_test, pred_labels_left, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd07eb39-906f-45af-9d54-914242e50a2c",
   "metadata": {},
   "source": [
    "**Now you can compare the classification results to our naive Bayes. Do you observe any improvement in the prediction accuracy?**\n",
    "\n",
    "We should also investigate the influence of the side of the padding and character selection on our prediction of the CNN. So please repeat the steps, but this time use a different character for the left padding. Feel free to use any character you like (with the exception of A, C, G and T). We will compare them later in the group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b12284-7fc8-429c-965e-75b827f0da1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad and ordinal encode the sequences\n",
    "full_df[\"seq_pad_left_alt\"] = full_df.sequence.str.rjust(600, XXXX)  # insert a different character in \"\" here\n",
    "full_df[\"ord_encoded_left_alt\"] = full_df.seq_pad_left_alt.map(lambda x: [ord(char) for char in x])\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, Y_train, Y_test = XXXX  # Don't forget to set the same random state as before\n",
    "\n",
    "# Transform Test and Train input to arrays\n",
    "X_train_array_left_alt = np.array(X_train[\"ord_encoded_left_alt\"].to_list()).reshape(4878, 600, -1)\n",
    "X_test_array_left_alt = np.array(X_test[\"ord_encoded_left_alt\"].to_list()).reshape(861, 600, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90083e11-240c-4495-80e2-eada287f40b4",
   "metadata": {},
   "source": [
    "Create the model and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e7df20-46ec-4f32-bb49-8d792b27996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the model\n",
    "model = create_cnn_model_1()\n",
    "history_left_alt = model.fit(X_train_array_left_alt, Y_train_ohe, XXXX)  # Enter the missing parameters from above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf97d870-b9c0-40a4-9d63-e9849056f6a1",
   "metadata": {},
   "source": [
    "Let's test the model and look at the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb4af5a-e670-4ff6-ac42-250753fada8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_left_alt = model.predict(XXXX)  # Predict the test set like above using the new model\n",
    "pred_labels_left_alt = ohe.inverse_transform(XXXX)  # Transform the predictions back into the labels \n",
    "\n",
    "print(classification_report(XXXX))  # Output the classification report with the true labels and the predicted labels\n",
    "\n",
    "# Save scores for plotting\n",
    "f1_left_alt = f1_score(Y_test, pred_labels_left_alt, average=\"weighted\")\n",
    "prec_left_alt = precision_score(Y_test, pred_labels_left_alt, average=\"weighted\")\n",
    "rec_left_alt = recall_score(Y_test, pred_labels_left_alt, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e75c33f-6d52-476a-be53-c7b83615ac39",
   "metadata": {},
   "source": [
    "\n",
    "So far we have padded the sequences solely on the left. \n",
    "\n",
    "Now also pad the sequence on the right using the `.ljust()` method. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb9880b-4470-471a-bd36-89c458d175c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sequences and ordinal encode them\n",
    "full_df[\"seq_pad_right\"] = full_df[XXXX].str.ljust(600, XXXX)  # Use one of the characters we used before and enter the sequence column\n",
    "full_df[\"ord_encoded_right\"] = full_df[XXXX].map(lambda x: [ord(char) for char in x])  # Enter the colemn we need to encode\n",
    " \n",
    "# Make sure you set the random state to the same number as before and choose the correct column for splitting. Stratify by the target variable\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(full_df[[XXXX]], full_df[\"type\"], test_size=0.15, random_state=XXXX, stratify=XXXX)\n",
    "\n",
    "# Transform Test and Train input into an array\n",
    "X_train_array_right = np.array(X_train[\"ord_encoded_right\"].to_list()).reshape(4878, 600, -1) \n",
    "X_test_array_right = np.array(X_test[XXXX].to_list()).reshape(861, 600, -1)  # Enter the column we want to transform to the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c48e01a-b91f-4d10-a115-baf0a154e707",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_cnn_model_1()\n",
    "model.fit(XXXX, XXXX, epochs=20, batch_size=40, verbose=2, validation_split=0.15)\n",
    "\n",
    "# Predict the test set and transform the prediction back into labels\n",
    "prediction_right = XXXX  # Predict the padding on the right side\n",
    "pred_labels_right = ohe.XXXX  # Transform our prediction back into the labels\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(XXXX))  # Enter the true labels and our predicted labels\n",
    "\n",
    "# Save scores for plotting later\n",
    "f1_right = f1_score(Y_test, pred_labels_right, average=\"weighted\")\n",
    "prec_right = precision_score(Y_test, pred_labels_right, average=\"weighted\")\n",
    "rec_right = recall_score(Y_test, pred_labels_right, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55db51a-d4e1-4ddb-878d-18191290b9aa",
   "metadata": {},
   "source": [
    "**How big is the infleunce of the padding side and character selection for or training? Can you imagine possible interpretations.**\n",
    "\n",
    "**Is the difference in the range of retraining the models?**\n",
    "\n",
    "As the nature of neural networks is non-deterministic (weights are randomly initialized) two models with the same configuration don't have to be exactly the same. Also the differences might also come from variances among the training/validation split.\n",
    "\n",
    "## Cross-Validation\n",
    "\n",
    "We want to embrace the randomness by splitting our training set into 5 equal parts, each of which will be the validation set one time and the training set the other times. This procedure is called (5-fold) [Cross Validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)). You create each model 5 times, then test it on the test set and average over the results. Normally, you would even want to do this at least 10 times for the k-fold cross validation or perform a leave one out ([LOO](https://machinelearningmastery.com/loocv-for-evaluating-machine-learning-algorithms/)) approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef9d6e2-c640-4052-acf5-e99e46f3c5c1",
   "metadata": {},
   "source": [
    "For our example we will use scikit-learn's [StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) to create five folds. \n",
    "\n",
    "For each fold we want to save the history of training, and the test scores. We will do this using dictionaries, where the keys are the encoding (\"left\", \"left_alt\" and \"right\") and the values are lists. You will be able to access history of the 2nd fold of the left padding model with `history_dict[\"left\"][1]`.\n",
    "\n",
    "You will not need to fill in any blank spaces in this block, but try to go through every line and understand what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d826ea-29f1-4a99-9bf9-30c78e708ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "kfold = StratifiedKFold()\n",
    "\n",
    "# We will use a dictionary to save our histories \n",
    "history_dict = {\"left\": [], \"left_alt\": [], \"right\": []}\n",
    "\n",
    "# We will use a dictionary to save the scores\n",
    "f1_dict = {\"left\": [], \"left_alt\": [], \"right\": []}\n",
    "\n",
    "# We will use dictionary to save the predictions\n",
    "pred_dict = {\"left\": [], \"left_alt\": [], \"right\": []}\n",
    "\n",
    "# Since the arrays were all created from the same train set, we can simply use the same splits for all three training sets\n",
    "# We will run another variable called i that is the number of the fold\n",
    "for i, (train_index, val_index) in zip(range(5), kfold.split(X_train_array_right, Y_train)):  \n",
    "    \n",
    "    print(f\"\\n\\nNow creating fold {i+1}\\n\\n\")  # +1 since python starts counting at 0\n",
    "    \n",
    "    print(f\"Running the left model\")\n",
    "    model_left = create_cnn_model_1() \n",
    "    # Fit the model and save it in the history dict\n",
    "    history_dict[\"left\"].append(model_left.fit(X_train_array_left[train_index], Y_train_ohe[train_index],\n",
    "                   epochs=20, batch_size=40, verbose=0,\n",
    "                   validation_data=(X_train_array_left[val_index], Y_train_ohe[val_index]))) \n",
    "    \n",
    "    # Predict the test data\n",
    "    prediction_left = model_left.predict(X_test_array_left)\n",
    "    # Inverse transform\n",
    "    pred_left_labels = ohe.inverse_transform(prediction_left)\n",
    "    # Save the prediction\n",
    "    pred_dict[\"left\"].append(pred_left_labels)\n",
    "    # Save the F1 score\n",
    "    f1_dict[\"left\"].append(f1_score(Y_test, pred_left_labels, average=\"weighted\"))\n",
    "    \n",
    "    # The same for the alternative encoding\n",
    "    print(f\"Running the left_alt model\")\n",
    "    model_left_alt = create_cnn_model_1()\n",
    "    history_dict[\"left_alt\"].append(model_left_alt.fit(X_train_array_left_alt[train_index], Y_train_ohe[train_index],\n",
    "                   epochs=20, batch_size=40, verbose=0,\n",
    "                   validation_data=(X_train_array_left_alt[val_index], Y_train_ohe[val_index])))\n",
    "    prediction_left_alt = model_left.predict(X_test_array_left_alt)\n",
    "    pred_left_alt_labels = ohe.inverse_transform(prediction_left_alt)\n",
    "    pred_dict[\"left_alt\"].append(pred_left_alt_labels)\n",
    "    f1_dict[\"left_alt\"].append(f1_score(Y_test, pred_left_alt_labels, average=\"weighted\"))\n",
    "    \n",
    "    # The same for the right model\n",
    "    print(f\"Running the right model\")\n",
    "    model_right = create_cnn_model_1()\n",
    "    history_dict[\"right\"].append(model_right.fit(X_train_array_right[train_index], Y_train_ohe[train_index],\n",
    "                   epochs=20, batch_size=40, verbose=0,\n",
    "                   validation_data=(X_train_array_right[val_index], Y_train_ohe[val_index])))\n",
    "    prediction_right = model_right.predict(X_test_array_right)\n",
    "    pred_right_labels = ohe.inverse_transform(prediction_right)\n",
    "    pred_dict[\"right\"].append(pred_right_labels)\n",
    "    f1_dict[\"right\"].append(f1_score(Y_test, pred_right_labels, average=\"weighted\"))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577c7258-0e18-494e-93b0-92bd5d0caa68",
   "metadata": {},
   "source": [
    "With the information gathered over the different models and the average we will be able to find more robust results of our performance.\n",
    "\n",
    "Sometimes a model may not learn any meaningful information, because it gets stuck in a local minimum right away. In this case, the mean value may be skewed, since it is succeptible to outliers. To avoid this issue, we will also check out the median values, which is robust against outliers. \n",
    "\n",
    "Lastly, we need to compare the single trained models to get information about the variance (in the form of a table). Transform the dictionary into a pandas Dataframe using `.from_dict()`. Does one epoch perform better than the others for all three models? What might be the reason for this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111fd7f4-3401-4b1f-9916-3b5d06eae9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # useful for printing mean and median scores\n",
    "\n",
    "# Print the mean F1-scores for each model\n",
    "print(f\"The mean F1-score for the left model is {np.mean(f1_dict['left'])}\")\n",
    "print(f\"The mean F1-score for the left_alt model is {np.mean(f1_dict[XXX])}\")  # Enter the model key to the dictionary\n",
    "print(f\"The mean F1-score for the right model is {XXXX}\\n\\n\")  # Perform the same for the right model\n",
    "\n",
    "# Print the median F1-score for each model\n",
    "print(f\"The median F1-score for the left model is {np.median(f1_dict['left'])}\")\n",
    "print(f\"The median F1-score for the left_alt model is {XXXX}\")  # Print the median for the alt encoding\n",
    "print(f\"The median F1-score for the right model is {XXXX}\\n\\n\")  # Print the median for the right model\n",
    "\n",
    "# Print the scores in the form of a table\n",
    "print(pd.DataFrame.from_dict(f1_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb02a0fa-f354-4301-8788-35962c5d6c6b",
   "metadata": {},
   "source": [
    "This values can be also visualized using seaborn to create a boxplot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc940ed-a367-4942-84e0-944410f32c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plot = sns.boxplot(data=pd.DataFrame.from_dict(f1_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a27ab10-0962-41e7-93ed-b218d195443e",
   "metadata": {},
   "source": [
    "**We can now identify the influence of padding as well as the selection of the character as placeholder**\n",
    "\n",
    "In contrast to our first classifier a neural network as well as CNNs are training over so called epochs. This means we should also track the val_loss and val_accuracy over time (epochs). So we can identify if the model is stucking from time to time in local optima or is constantly learning.\n",
    "\n",
    "Therefore we plot the loss and accuracy of the best model throughout every epoch. You can also compare if the padding \"left\", \"right\", \"left_alt\" showes differences in their learning over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5608fc-0fdc-4f7d-a978-2999045086b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = plt.subplot()\n",
    "\n",
    "# Iterate over every fold\n",
    "for i in range(5):\n",
    "    # Plot Validation scores\n",
    "    # Feel free to change the \"left\" column to the best scoring model\n",
    "    ax.plot(history_dict[\"left\"][i].history['val_accuracy'], label=f'val acc {i}', color=\"orange\", alpha=0.4)\n",
    "    # Put in the same column as above\n",
    "    ax.plot(history_dict[XXXX][i].history['val_loss'], label=f'val loss {i}', color=\"orange\", alpha=0.4)  \n",
    "    \n",
    "    # Plot Training scores\n",
    "    ax.plot(history_dict[XXXX][i].history['accuracy'], label=f'acc {i}', color=\"blue\", alpha=0.4)\n",
    "    ax.plot(history_dict[XXXX][i].history['loss'], label=f'loss {i}', color=\"blue\", alpha=0.4)\n",
    "\n",
    "\n",
    "ax.set_ylim(0,1.1)\n",
    "ax.grid(True)\n",
    "plt.title(\"Loss and Accuracy for the best model\\nBlue: Training; Orange: Validation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658aace9-8b72-47fd-a8a6-7966831d6329",
   "metadata": {},
   "source": [
    "# Optimize the training of the initial CNN model \n",
    "\n",
    "At stage we used sequential features in Notebook 1 and now the sequence translated as integers. \n",
    "\n",
    "**Can we assume that the CNN is smart enough not to create a bias?**\n",
    "\n",
    "For example: \"A\" is encoded as 1 and \"T\" is encoded as 4. Is the CNN now interpreting \"T's\" as for times more important as \"A's\"? Is this a biologically sensful approach to create an order of nucelotides like A\\<C\\<G\\<T?\n",
    "\n",
    "For removing such probable weighting we will now perform an encoding of the input beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa740a25-d6f3-4898-b883-a4106e030010",
   "metadata": {},
   "source": [
    "## Remove Weighting of Nucleotides \n",
    "\n",
    "Luckily, we have already used a method for creating equally weighted numerical encoding. Remember when we encoded the output labels? We used one-hot encoding to encode \"CD-Box\" as `[1, 0, 0]`, \"HACA-Box\" as `[0, 1, 0]` and \"scaRNA\" as `[0, 0, 1]`. We could do the same for the nucleotides. \n",
    "\n",
    "Create a new `OneHotEncoder` (not sparse) but add `handle_unknown=\"ignore\"`. If one of the other [IUPAC codes](https://droog.gs.washington.edu/parc/images/iupac.html) for nucleotides snuck into our sequences, this will make sure that they do not create a new row for each of the inputs. \n",
    "\n",
    "We will use `.map()` with a `lambda` function to transform the ordinal encoded sequences in the original `full_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18c32ad-39f6-4de0-bf9d-f38f0f946bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the one hot encoder\n",
    "ohe_seq_1 = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n",
    "\n",
    "# Fit the one-hot encoder to the data\n",
    "ohe_seq_1.fit(np.array(full_df[\"ord_encoded_left\"][0]).reshape(-1, 1))\n",
    "\n",
    "# Create a new column with the transformed one-hot encoded input\n",
    "full_df[\"seq_ohe\"] = full_df[\"ord_encoded_left\"].map(lambda x: ohe_seq_1.transform(np.array(x).reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cccb1e3-2d31-4054-bc5b-f955d9384132",
   "metadata": {
    "tags": []
   },
   "source": [
    "The last expression may look a little tricky, but this is again because we are handling different datatypes and -shapes.\n",
    "\n",
    "The encoded sequences have now the following shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33256ff3-ac12-449b-9e62-e3fa1d7f3841",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_df[\"seq_ohe\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a71c3e-96b6-4c6c-8d6f-1fabf4678f70",
   "metadata": {},
   "source": [
    "The encoding is `A = [1, 0, 0, 0, 0]`, `C = [0, 1, 0, 0, 0]`, `G = [0, 0, 1, 0, 0]`, `T = [0, 0, 0, 1, 0]` and `_ = [0, 0, 0, 0, 1]`.\n",
    "\n",
    "Let's split our data into train and test again. Make sure you use the same `random_state` as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cae00c-fa0f-452f-ab11-83051691baa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random state to the same number as before\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(full_df[[\"seq_ohe\"]], full_df[\"type\"], test_size=0.15, random_state=XXXX, stratify=full_df[\"type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a36ffa-426a-4233-8064-30877574e327",
   "metadata": {},
   "source": [
    "Put the data into an array again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d93bc1-170c-4c95-a896-3cd82243e1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_array_ohe = np.array(X_train.seq_ohe.to_list()).reshape(len(X_train), 600, 5)\n",
    "X_test_array_ohe = np.array(X_test.seq_ohe.to_list()).reshape(len(X_test), 600, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1e8ba8-5fbf-48b8-9d3b-2275874c45f1",
   "metadata": {},
   "source": [
    "We will not be needing the full_df anymore after this, let's delete it to free up some RAM. We can use the Python [Garbace Collector](https://docs.python.org/3/library/gc.html) to make sure, the RAM is freed immediately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8940cf68-b96b-48d9-a628-958c568b22f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "del full_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bd5668-778a-4e39-bc08-1a043851be15",
   "metadata": {},
   "source": [
    "The input dimensions of our data has changed slightly. Instead of `input_shape=(600, 1)`, we will now have to specify `(600, 5)`, to account for the four additional rows in each input. Other than that, we will use the same model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396cfa4c-0dbf-427b-a3ba-c10816a460de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_cnn_model_2():\n",
    "    \n",
    "    # create the model\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Enter the values for activation and padding like in the previous models\n",
    "    model.add(keras.layers.Conv1D(32, 7, activation=XXXX, padding=XXXX, input_shape=(600, 5)))\n",
    "    model.add(keras.layers.MaxPooling1D(4))\n",
    "    model.add(keras.layers.Dropout(rate = 0.5))\n",
    "    model.add(keras.layers.Flatten())\n",
    "\n",
    "    model.add(keras.layers.Dense(3, activation=\"softmax\")) \n",
    "\n",
    "    Adam = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=Adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab5b50f-4297-4a53-bf7d-15c6976d1532",
   "metadata": {},
   "source": [
    "\n",
    "Create an instance of the new model and train it on the one-hot encoded training array. This time, let's train a little longer. Train for 40 epochs on a batch size of 40 and with a validation split of 0.15.\n",
    "\n",
    "We want to look at the training afterwards in more detail, so lets save the training as a history object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b64667-8ff2-4ca4-b759-dcce2785edac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the model to the training data\n",
    "model = create_cnn_model_2()\n",
    "history = model.fit(X_train_array_ohe, Y_train_ohe, epochs=60, batch_size=XXXX, verbose=2, validation_split=XXXX) # Enter the missing parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5a9000-527e-4bd8-a2b1-baceddf4a1e9",
   "metadata": {},
   "source": [
    "Let's look at results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7d13f3-2ef9-4b69-892c-15f8a5ba0d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test set and transform into labels\n",
    "prediction_ohe = model.predict(X_test_array_ohe)\n",
    "pred_labels_ohe = ohe.inverse_transform(prediction_ohe)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(Y_test, pred_labels_ohe))\n",
    "\n",
    "# Save scores for plotting\n",
    "f1_ohe = f1_score(Y_test, pred_labels_ohe, average=\"weighted\")\n",
    "prec_ohe = precision_score(Y_test, pred_labels_ohe, average=\"weighted\")\n",
    "rec_ohe = recall_score(Y_test, pred_labels_ohe, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be80cf30-1c38-471d-b373-09867ba96ef3",
   "metadata": {},
   "source": [
    "The scores like precision, recall and the F1-score are all improved removing the weighting of the nucleotides.\n",
    "\n",
    "Let's look at the training. We will use matplotlib.pyplot for this and visualize the history over the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e3d486-59b5-4fe7-9e6b-9016a1584558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = plt.subplot()\n",
    "\n",
    "# Plot validation accuracy and loss\n",
    "ax.plot(history.history['val_accuracy'], label=f'val acc', color=\"orange\", alpha=1)\n",
    "ax.plot(history.history['val_loss'], label=f'val loss', color=\"red\", alpha=1)\n",
    "\n",
    "# Plot training accuracy and loss\n",
    "ax.plot(history.history['accuracy'], label=f'acc', color=\"blue\", alpha=1)\n",
    "ax.plot(history.history['loss'], label=f'loss', color=\"green\", alpha=1)\n",
    "\n",
    "\n",
    "ax.set_ylim(0,1.1)\n",
    "ax.grid(True)\n",
    "plt.legend()\n",
    "plt.title(\"Validation Loss and Accuracy for the One-Hot Encoding\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3922de2-f90f-46e9-a034-c667fd74839e",
   "metadata": {},
   "source": [
    "**Changing the length of the training has an influence on the accuracy of our model. To create the optimal model we would like to stop automatically if the model is not further improving for some time.**\n",
    "\n",
    "This approach is called early stopping. Keras provides a class called [EarlyStopping](https://keras.io/api/callbacks/early_stopping/) that you can use to save the best model. Let's create an early stopping monitor and run the model again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418a34f2-9549-4701-b23a-e218a6c38ec2",
   "metadata": {},
   "source": [
    "We want our Early Stopping to monitor `val_loss`, since this is the metric we evaluate the model on. `patience` is a way to make sure you don't train unnecessarily. If patience is set to 10, the model will stop training after 10 consecutive epochs, where the validation loss has not improved from the previous minimum. Lastly, make sure `restore_best_weights` is set to true, as this means, that the output model will save the weights with the lowest validation loss instead of the last training epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8e1b64-950b-45aa-8cbd-110fb24b18c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ef1338-3e89-4c85-8d04-c4faaf06e1c8",
   "metadata": {},
   "source": [
    "Now let's train the model again with the Early Stopping Monitor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb52ac8e-5f13-45b0-8502-2d626b01b63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the model to the test data\n",
    "model = create_cnn_model_2()\n",
    "# Fill in the missing inputs and parameters\n",
    "history = model.fit(XXXX, XXXX, epochs=60, batch_size=XXXX, verbose=2, validation_split=XXXX, callbacks=[callback]) \n",
    "\n",
    "# Predict the test set and transform the prediction back to labels\n",
    "prediction_ohe_es = model.predict(XXXX)\n",
    "pred_labels_ohe_es = ohe.inverse_transform(XXXX)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(Y_test, pred_labels_ohe_es))\n",
    "\n",
    "# Save scores for later plotting\n",
    "f1_ohe_es = f1_score(Y_test, pred_labels_ohe_es, average=\"weighted\")\n",
    "prec_ohe_es = precision_score(Y_test, pred_labels_ohe_es, average=\"weighted\")\n",
    "rec_ohe_es = recall_score(Y_test, pred_labels_ohe_es, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562e492b-dafa-4625-b561-e5758bb17607",
   "metadata": {},
   "source": [
    "**Have the predictions improved?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2e6cd0-e94d-44f9-99ee-310122f91c5f",
   "metadata": {},
   "source": [
    "## Changing the model architecture and the convolutions\n",
    "\n",
    "So far, we have used a fixed number of filters (32) and size (7). Changing this two parameters is essential to find the optimal solution.\n",
    "\n",
    "We will create a new model with the same architecture, but setting these two parameters as flexibel variables that can be provided during model creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780b95f2-7e57-445a-88b2-497a8bf9fa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model_3(filters=32, size=7):\n",
    "    \n",
    "    # create the model\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    model.add(keras.layers.Conv1D(filters, size, activation='relu', padding='same', input_shape=(600, 5)))\n",
    "    model.add(keras.layers.MaxPooling1D(4))\n",
    "    model.add(keras.layers.Dropout(rate = 0.5))\n",
    "    model.add(keras.layers.Flatten())\n",
    "\n",
    "    model.add(keras.layers.Dense(3, activation=\"softmax\")) \n",
    "\n",
    "    Adam = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=Adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae9538a-6e0d-42ff-93de-b8c0b4aa6e91",
   "metadata": {},
   "source": [
    "For parameter optimization we will use [Grid Search](https://medium.com/fintechexplained/what-is-grid-search-c01fe886ef0a). In summary, grid search uses brute force to test every combination of parameters to determine which one has the highest scores.\n",
    "\n",
    "We will use scikit-learn's `GridSearchCV` class ([doc](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)). To be able to enter our model into a scikit-learn model, we will have to wrap it using KerasClassifier()\\*\n",
    "\n",
    "\\*Note, that this method is deprecated. We will use it for the moment for simplicity, but note that in a year it might no longer be supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae3ca98-78e7-43ef-8a47-40981d0eed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = KerasClassifier(build_fn=create_cnn_model_3)  # build_fn is the function used to create the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ede941-915b-44fa-81db-d4727a314b9a",
   "metadata": {},
   "source": [
    "Now we have to specify the parameter combinations we want to test. This is done using a dictionary. Note, that the keys will have to have the same name as the parameter variables and the values will be lists of numbers you want to test. \n",
    "\n",
    "**Select three values of filter and size.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf9741-a15f-4aa4-b3ea-35cd98983b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dictionary with parameters we want to iterate over\n",
    "param_grid = {\"filters\": [32, 64, 128], \"size\": [7, 11]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a5068e-5a86-4351-a987-c195ad78223a",
   "metadata": {},
   "source": [
    "Now we are ready to create an instance of `GridSearchCV`. As you have seen above, it is not sensible to just run a model once, so make sure you set the cross-validation `cv` parameter to 5 or higher. Normally, we would use this `cv` but for today we will only perform 2 cross-validations. In the `.fit()` call, set the epochs to 20  and set `verbose` to 0, as there will be a lot of outputs. \n",
    "\n",
    "Notice that the grid search will take a couple of minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4d17a6-3514-4647-bb00-b9c2a08108df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the grid search \n",
    "gs = GridSearchCV(estimator=model, param_grid=param_grid, cv=2)\n",
    "\n",
    "# Fit the grid search to the training set (parameters are the same from the regular fit calls from the keras models)\n",
    "gs = gs.fit(XXXX, XXXX, epochs=25, batch_size=XXXX, verbose=0, validation_split=0.15)  # fill in the missing input and parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd243f3c-33cb-4a77-afb9-f1af919872ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs.best_params_)\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68606a8-d76c-49ff-af13-41a379d70cc4",
   "metadata": {},
   "source": [
    "**Which values gave the best score for filter and size in combination?**\n",
    "\n",
    "Since the optimal combination could be different from our initial setup we would normally test more filter and size values in the grid search to find the best solution. \n",
    "\n",
    "In our case we will now proceed further with the training of our model with the adjusted parameters and make use of the early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c501827-4a0a-4821-b64d-7e125ab1e33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Create the model\n",
    "model = create_cnn_model_3(filters=XXXX, size=XXXX)  # Enter the best parameter combination\n",
    "\n",
    "# Enter the missing input and parameters for fitting\n",
    "history = model.fit(XXXX, XXXX, epochs=60, batch_size=XXXX, verbose=2, validation_split=0.15, callbacks=[callback])\n",
    "\n",
    "# Predict the test set\n",
    "prediction_ohe_gs = model.predict(XXXX)\n",
    "\n",
    "# Transform prediction back to labels\n",
    "pred_labels_ohe_gs = ohe.inverse_transform(prediction_ohe_gs)\n",
    "\n",
    "# Create the classification report\n",
    "print(classification_report(XXXX))\n",
    "\n",
    "# Save scores for plotting\n",
    "f1_ohe_gs = f1_score(Y_test, pred_labels_ohe_gs, average=\"weighted\")\n",
    "prec_ohe_gs = precision_score(Y_test, pred_labels_ohe_gs, average=\"weighted\")\n",
    "rec_ohe_gs = recall_score(Y_test, pred_labels_ohe_gs, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fb4e67-ec81-4d38-a417-53fa7ff664c7",
   "metadata": {},
   "source": [
    "\n",
    "So far we have only used a single convolutional block in our models but for sure we could also create blocks of convolution, maxpooling and dropout layers. Let's use two blocks of layers for our next model and see if that changes the results for our specific example problem. We will use the optimized filters in the first block and one problem in deeper CNNs is then to find the optimal combination of parameter in the following blocks. In our case we decided to make the size and filter smaller compared to the first block to identify more fine-grained features (this is based on theory and can be different in practice). Normally, we would now have to optimize over these filters in combination with the first ones again, but we will skip that due to time constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9093a3d7-588b-4c4b-b2d8-163ace28bf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model_5():\n",
    "    \n",
    "    # create the model\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    model.add(keras.layers.Conv1D(128, 11, activation='relu', padding='same', input_shape=(600, 5)))\n",
    "    model.add(keras.layers.MaxPooling1D(4))\n",
    "    model.add(keras.layers.Dropout(rate = 0.5))\n",
    "    \n",
    "    # Enter the missing parameters. You can choose the same as the first convolutional block, or you can try fewer/smaller filters\n",
    "    model.add(keras.layers.Conv1D(XXXX, XXXX, activation='relu', padding='same'))  \n",
    "    model.add(keras.layers.MaxPooling1D(XXXX)) # Set the pool size \n",
    "    model.add(keras.layers.Dropout(rate = 0.5))\n",
    "    model.add(keras.layers.Flatten())\n",
    "\n",
    "    model.add(keras.layers.Dense(3, activation=\"softmax\")) \n",
    "\n",
    "    Adam = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=Adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3607ef27-3c33-492a-95c2-a089fa5d009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Create and fit the model\n",
    "model = create_cnn_model_5()\n",
    "history = model.fit(XXXX, XXXX, epochs=60, batch_size=XXXX, verbose=2, validation_split=0.15, callbacks=[callback])\n",
    "\n",
    "# Predict the test set and transform the prediction back to the labels\n",
    "prediction_ohe_deep = model.predict(XXXX)\n",
    "pred_labels_ohe_deep = ohe.inverse_transform(prediction_ohe_deep)\n",
    "\n",
    "# Print classifcation report\n",
    "print(classification_report(Y_test, pred_labels_ohe_deep))\n",
    "\n",
    "# Save scores for plotting\n",
    "f1_deep = f1_score(Y_test, pred_labels_ohe_deep, average=\"weighted\")\n",
    "prec_deep = precision_score(Y_test, pred_labels_ohe_deep, average=\"weighted\")\n",
    "rec_deep = recall_score(Y_test, pred_labels_ohe_deep, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c1ef37-a125-4586-88d4-8f004c42d8fd",
   "metadata": {},
   "source": [
    "After adjusting a lot of optimization to our CNN model we should not forget to keep our biological question in the focus point. So we would like to see now beside the F1-score how good can our CNN predict the different ncRNA classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ddabbb-87d0-411a-a356-807445b94eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrix = confusion_matrix(Y_test, pred_labels_ohe_deep, normalize=\"true\")\n",
    "# Turn into a dataframe (necessary for plotting)\n",
    "conf_df = pd.DataFrame(conf_matrix, index=[\"CD-Box\", \"HACA-Box\", \"scaRNA\"], columns=[\"CD-Box\", \"HACA-Box\", \"scaRNA\"])\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(conf_df, annot=True, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad2cbe5-8d8b-4480-9ca8-992dc6cfc752",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot()\n",
    "\n",
    "# Plot the validation scores\n",
    "ax.plot(history.history['val_accuracy'], label=f'val acc', color=\"orange\", alpha=1)\n",
    "ax.plot(history.history[XXXX], label=f'val loss', color=\"red\", alpha=1)\n",
    "\n",
    "# Plot the training scores\n",
    "ax.plot(history.history[XXXX], label=f'acc', color=\"blue\", alpha=1)\n",
    "ax.plot(history.history[XXXX], label=f'loss', color=\"green\", alpha=1)\n",
    "\n",
    "\n",
    "ax.set_ylim(0,1.1)\n",
    "ax.grid(True)\n",
    "plt.legend()\n",
    "plt.title(\"Loss and Accuracy for the 'Deep' model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b30b4d-7154-463f-a330-2472646bf122",
   "metadata": {},
   "source": [
    "Plot the saved scores from all CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd3ad73-1a79-4860-b38a-bb52d82d7ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scores in a dictionary\n",
    "dictionary = {\"F1\": [f1_left, f1_right, f1_left_alt, f1_ohe, f1_ohe_es, f1_ohe_gs, f1_deep],\n",
    "              \"Precision\": [prec_left, prec_right, prec_left_alt, prec_ohe, prec_ohe_es, prec_ohe_gs, prec_deep],\n",
    "              \"Recall\": [rec_left, rec_right, rec_left_alt, rec_ohe, rec_ohe_es, rec_ohe_gs, rec_deep],\n",
    "              \"Model\": [\"Left Encoding\", \"Right Encoding\", \"Alt Left Encoding\", \"OHE\", \"Grid Search\", \"Early Stopping\", \"Deep\"]}\n",
    "# Turn into a dataframe\n",
    "df = pd.DataFrame(dictionary)\n",
    "df = pd.melt(df, id_vars=[\"Model\"])\n",
    "\n",
    "# Plot Barplot with scores\n",
    "plot = sns.barplot(data=df, x=\"variable\", y=\"value\", hue=\"Model\")\n",
    "plt.legend(loc='lower right')\n",
    "plot.set(ylim=(0.6, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff14251-376e-4ced-8b41-96137c6c63a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Try to find the perfect solution for the problem\n",
    "\n",
    "CNN creation takes a lot of intuition, experience but also some trial and error. \n",
    "Most of the times you don't need to change a lot of hyperparameter and combinations to see if the problem can be solved successfully but for optimizing hte last percentages in your model this can be consume quite some time. Many combinations of hyperparameters and architectures will only lead to marginal classification differences. \n",
    "\n",
    "So far, we have not talked about many additional hyperparameters including different activation functions and padding, but you may also look at the Keras documentation to see, what other options are available.\n",
    "\n",
    "This last box will be now allowing you for the next 10-15 minutes to test your model of choice and see if you can improve the classification further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0702df5b-74b0-4353-a7f7-4b72607279fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model_6():\n",
    "\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Set the parameters to a combination you want to test\n",
    "    model.add(keras.layers.Conv1D(XXX, XXX, activation='relu', padding='same', input_shape=(600, 5)))\n",
    "    model.add(keras.layers.MaxPooling1D(XXX))\n",
    "    model.add(keras.layers.Dropout(rate=XXX))\n",
    "    \n",
    "    # How about a second block of layers? \n",
    "    model.add(keras.layers.Conv1D(XXX, XXX, activation='relu', padding='same'))\n",
    "    model.add(keras.layers.MaxPooling1D(XXX))\n",
    "    model.add(keras.layers.Dropout(rate=XXX))\n",
    "    \n",
    "    # Even more convolutions? You can also try to switch up the structure of the blocks\n",
    "    # maybe try to add a second convolutional layer before the MaxPooling step\n",
    "    \n",
    "    # DO NOT CHANGE\n",
    "    model.add(keras.layers.Flatten())\n",
    "    \n",
    "    # Often times, convolutional layers have a \"Dense Head\", i.e. multiple Dense layers before the output. Maybe this improves classification\n",
    "    model.add(keras.layers.Dense(XXX, activation=\"relu\"))\n",
    "    \n",
    "    # DO NOT CHANGE\n",
    "    model.add(keras.layers.Dense(3, activation=\"softmax\"))\n",
    "\n",
    "    # Test different optimizers\n",
    "    optimizer1 = keras.optimizers.Adam(learning_rate=XXX)\n",
    "    optimizer2 = keras.optimizers.SGD(learning_rate=XXX)\n",
    "    optimizer3 = keras.optimizers.XXX(learning_rate=XXX)\n",
    "    \n",
    "    model.compile(optimizer=XXX, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9db1c1d-cb52-4670-b48e-8e9e00e179e0",
   "metadata": {},
   "source": [
    "Now create and fit your model. Keep in mind, that [learning rate and batch size are related](https://www.baeldung.com/cs/learning-rate-batch-size), so if you change one of them you would usually want to change the other one as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaf8f2a-55a9-41c5-bc68-d6cb04426469",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model_6()\n",
    "model.fit(X_train_array_ohe, Y_train_ohe, epochs=XXX, batch_size=XXX, verbose=2, validation_split=0.15)\n",
    "\n",
    "prediction_your_model = model.predict(X_test_array_ohe)\n",
    "\n",
    "pred_labels_your_model = ohe.inverse_transform(prediction_your_model)\n",
    "print(classification_report(Y_test, pred_labels_your_model, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b0903b-3989-4532-a224-c3e2bb8be109",
   "metadata": {},
   "source": [
    "# Creating and including more information from the initial sequences \n",
    "\n",
    "While we could probably get better accuracy through further optimization, we also have the option to change the data input for the classification problem. In our case we can use the initial sequences of ncRNAs and predict from them the secondary structure of the sequence. As the function of snoRNAs is generally defined by their secondary structure this could gain more information. \n",
    "\n",
    "## See you in 3_cnn_sec_struct.ipynb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
