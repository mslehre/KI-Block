{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEFORE YOU CONTINUE**: If you appear to be running out of RAM (top right bar) you can shut down the kernels of the 1st and 2nd notebook. This should free up some space. Shut down the kernels by selecting \"Running terminals and kernels\" (on the left side of the screen, the circle with the square in the middle). Then, within the KERNELS menu press the x next to the kernel of the first or second notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 CNN Prediction using secondary structure\n",
    "\n",
    "The secondary structure of RNA sequences can be predicted from the initial nucleotide sequence. Roughly summarized each nucleotide has a tendency to be paired and has preferences to bind to another nucleotide. These rules can be used to predict based on thermal dynamics the optimal secondary structure. One popular tool to predict the secondary structure in silico is the method \"RNAfold\" by ViennaRNA ([PubMed](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2447809/)). The secondary structure is usually represented as a dot-bracket notation, which encodes the structure using the \".\", \"(\" and \")\" characters, where \".\" represents an unpaired nucleotide, and a pair of opening and closing brackets encodes two nucleotides, that are paired with each other.\n",
    "\n",
    "We now have two \"channels\" of our input (sequence and structure), similarly to how an image has the three RGB channels, we are hoping to see improvements in the prediction, similarly to how an image classifier might have better results when trained on colored pictures instead of grey-scale. We will try to introduce the two channels after first using just the dot-bracket code but also test a the inclusion of a priori knowledge by annotating the dot-bracket code depending on specific patterns of secondary structures like several loops. To this end, the tool [Pysster](https://pubmed.ncbi.nlm.nih.gov/29659719/) uses \"RNAfold\" for the structure prediction and annotates substructures (F: 5'-end; S: Stem; H: Hairpin Loop; M: Multiloop; I: Internal Loop; and T: 3'-end) based on the dot-bracket notation.\n",
    "\n",
    "Example:\n",
    "\n",
    "Nucleotide Sequence\n",
    "\n",
    "`GTGCCTTTTAAGGCTGATGCAGTGCTTTAAGAGGCTAACACTGAAGGGTAAAGAAAACCATAAAACCCAGAGAAGAGATTGCAAAACTCCTCTTTGAATCCTGTCTGGATTCAAAGCT`\n",
    "\n",
    "Dot-Bracket Notation\n",
    "\n",
    "`..((((((((((((.........))))))))))))..........((((...............))))......(((.((....)))))..((((((((((.....))))))))))..`\n",
    "\n",
    "With a priori knowledge annotated substructures\n",
    "\n",
    "`FFSSSSSSSSSSSSHHHHHHHHHSSSSSSSSSSSSMMMMMMMMMMSSSSHHHHHHHHHHHHHHHSSSSMMMMMMSSSISSHHHHSSSSSMMSSSSSSSSSSHHHHHSSSSSSSSSSTT`\n",
    "\n",
    "---\n",
    "\n",
    "## Read in the data\n",
    "\n",
    "We want to compare how well we can predict the snoRNA type using the different secondary structure annotations. Let's start by reading in the annotations, you can find them in `snoRNA_secondary_structure.txt`. The file has the following format\n",
    "\n",
    "`>sequence_id` \n",
    "\n",
    "`snorna_type`\n",
    "\n",
    "`nucleotide sequence`\n",
    "\n",
    "`dot-bracket-notation`\n",
    "\n",
    "`structure annotation`\n",
    "\n",
    "Example:\n",
    "\n",
    "`>URS000217EFFD_55149`\n",
    "\n",
    "`HACA-Box`\n",
    "\n",
    "`GTGCCTTTTAAGGCTGATGCAGTGCTTTAAGAGGCTAACACTGAAGGGTAAAGAAAACCATAAAACCCAGAGAAGAGATTGCAAAACTCCTCTTTGAATCCTGTCTGGATTCAAAGCT`\n",
    "\n",
    "`..((((((((((((.........))))))))))))..........((((...............))))......(((.((....)))))..((((((((((.....))))))))))..`\n",
    "\n",
    "`FFSSSSSSSSSSSSHHHHHHHHHSSSSSSSSSSSSMMMMMMMMMMSSSSHHHHHHHHHHHHHHHSSSSMMMMMMSSSISSHHHHSSSSSMMSSSSSSSSSSHHHHHSSSSSSSSSSTT`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a data frame to read in the different sequence information\n",
    "\n",
    "To read in the data input, create five empty lists to which you can append the entries for each sample. The first list, `id_list`, will be for the sequence IDs, the second list ,`type_list`, will be for the snoRNA type, the third list, `seq_list`, will be for the nucleotide sequences, the fourth list is called `dotbrack_list` and contains the dot-bracket-notation, and lastly, the list `structure_list` is for the sequence of structure annotations. \n",
    "\n",
    "We will read the file in, by iterating line by line while having a counter variable count the line we are on. Since there are 5 lines for each entry, we can simply divide the length of the file by 5 and access each sequence by iterating to this number with the variable `i`. the `i`-th line will then be the ID, the `i+1`-th line will be the type, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "structure_file = open(\"snorna_seq_dotbrack_structure.txt\")\n",
    "lines = structure_file.readlines()\n",
    "n_lines = len(lines)\n",
    "n_sequences = int(n_lines/5)\n",
    "\n",
    "id_list = []\n",
    "type_list = []\n",
    "seq_list = []\n",
    "dotbrack_list = []\n",
    "structure_list = []\n",
    "\n",
    "# iteration through the file in steps of five\n",
    "for i in range(n_sequences):\n",
    "    id_list.append(lines[i*5].strip(\">\").strip(\"\\n\")) \n",
    "    type_list.append(lines[i*5+1].strip(\"\\n\"))\n",
    "    seq_list.append(lines[i*5+2].strip(\"\\n\"))\n",
    "    dotbrack_list.append(lines[i*5+3].strip(\"\\n\"))\n",
    "    structure_list.append(lines[i*5+4].strip(\"\\n\"))\n",
    "\n",
    "# Form a dictionary from the lists\n",
    "struct_dict = {\"id\": id_list, \"type\": type_list, \"sequence\": seq_list, \"dot_bracket\": dotbrack_list, \"structure\": structure_list}\n",
    "\n",
    "# Transform into a DataFrame\n",
    "struct_df = pd.DataFrame.from_dict(struct_dict)\n",
    "\n",
    "# Close the file\n",
    "structure_file.close()\n",
    "                   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same model architecture from before here but feel free to adapt it to your optimized version. \n",
    "Our next models will all be tested on the same architecture and the only adjustment from our previous architecture will be the size of the second input dimension (to account for different encodings). \n",
    "\n",
    "We will create our CNN model to easily adjust the second input dimension (the number of rows in each one-hot-encoded vector) as an input parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "def create_cnn_model_struct(input_dim_2):\n",
    "\n",
    "    # create the model\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # You may adapt the parameters to the optimized ones from the last notebook\n",
    "    model.add(keras.layers.Conv1D(128, 11, activation='relu', padding='same', input_shape=(600, input_dim_2)))\n",
    "    model.add(keras.layers.MaxPooling1D(4))\n",
    "    model.add(keras.layers.Dropout(rate = 0.5))\n",
    "    \n",
    "    # You may adapt the parameters to the optimized ones from the last notebook\n",
    "    model.add(keras.layers.Conv1D(64, 7, activation='relu', padding='same'))\n",
    "    model.add(keras.layers.MaxPooling1D(4))\n",
    "    model.add(keras.layers.Dropout(rate = 0.5))\n",
    "    \n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(3, activation=\"softmax\"))\n",
    "\n",
    "    Adam = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=Adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will have to perform the same preprocessing steps for the dot-bracket notation (padding, ordinal encoding, one-hot-encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np \n",
    "\n",
    "# Pad and ordinal encode the dot bracket notation\n",
    "struct_df[\"db_pad\"] = struct_df.dot_bracket.str.rjust(600, \"_\")\n",
    "struct_df[\"db_pad_ord\"] = struct_df.db_pad.map(lambda x: [ord(c) for c in x])\n",
    "\n",
    "# One-hot encode the dot-bracket\n",
    "ohe_seq_1 = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n",
    "ohe_seq_1.fit(np.array(struct_df.db_pad_ord[0]).reshape(-1, 1))\n",
    "struct_df[\"db_ohe\"] = struct_df.db_pad_ord.map(lambda x: ohe_seq_1.transform(np.array(x).reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need numbers again after this. Set the seed to the same number as in the previous notebook in the following cell. We will use the same random seed for the entire notebook, so don't change it again after setting it once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random = XXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(struct_df[[\"db_ohe\"]], struct_df[\"type\"], test_size=0.15, random_state=random, stratify=struct_df[\"type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode the labels\n",
    "ohe = OneHotEncoder(sparse=False)  # Create an instance of the OneHotEncoder class. sparse=True would be useful for instances with a lot of different labels, but for us we don't need it\n",
    "ohe.fit(np.array(Y_train).reshape(-1, 1))  # Fit the encoder to our data\n",
    "Y_train_ohe = ohe.transform(np.array(Y_train).reshape(-1, 1))  # Transform the training labels\n",
    "Y_test_ohe = ohe.transform(np.array(Y_test).reshape(-1, 1))  # Transform the test labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform input into arrays. Since there are four different characters, `.`, `(`, `)` and `_`, the second dimension will be four.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn train an\n",
    "X_train_db_array = np.array(X_train.db_ohe.to_list()).reshape(len(X_train), 600, 4)\n",
    "X_test_db_array = np.array(X_test.db_ohe.to_list()).reshape(len(X_test), 600, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate the prediction accuracy of the dot-bracket CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Create and fit the model to the train data\n",
    "model = create_cnn_model_struct(input_dim_2=XXXX)  # Set the input dim to the length of our OneHotEncoded vectors\n",
    "model.fit(X_train_db_array, Y_train_ohe, epochs=40, batch_size=40, verbose=2, validation_split=0.15, callbacks = [callback])\n",
    "\n",
    "# Predict the test data and transform the prediction to labels\n",
    "pred = model.predict(X_test_db_array)\n",
    "pred_labels_db = ohe.inverse_transform(pred)\n",
    "\n",
    "# Save scores for plotting\n",
    "f1_db = f1_score(Y_test, pred_labels_db, average=\"weighted\")\n",
    "prec_db = precision_score(Y_test, pred_labels_db, average=\"weighted\")\n",
    "rec_db = recall_score(Y_test, pred_labels_db, average=\"weighted\")\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(Y_test, pred_labels_db, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2304"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "del X_train_db_array, X_test_db_array\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to train on the structure string and compare it to the results from the dot-bracket string.\n",
    "\n",
    "The steps will be the same, padding, ordinal encoding, one-hot encoding, train/test splitting and transforming into an array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_df[\"struct_pad\"] = struct_df.structure.str.rjust(600, \"_\")\n",
    "struct_df[\"struct_pad_ord\"] = struct_df.struct_pad.map(lambda x: [ord(c) for c in x])\n",
    "\n",
    "ohe_seq_2 = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n",
    "ohe_seq_2.fit(np.array(struct_df.struct_pad_ord[0]).reshape(-1, 1))\n",
    "struct_df[\"struct_ohe\"] = struct_df.struct_pad_ord.map(lambda x: ohe_seq_2.transform(np.array(x).reshape(-1,1)))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(struct_df[[\"struct_ohe\"]], struct_df[\"type\"], test_size=0.15, random_state=random, stratify=struct_df[\"type\"])\n",
    "X_train_struct_array = np.array(X_train.struct_ohe.to_list()).reshape(len(X_train), 600, 6)\n",
    "X_test_struct_array = np.array(X_test.struct_ohe.to_list()).reshape(len(X_test), 600, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the better approach to solve our problem?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Create and fit the model to the train data\n",
    "model = create_cnn_model_struct(input_dim_2=6)\n",
    "model.fit(X_train_struct_array, Y_train_ohe, epochs=50, batch_size=40, verbose=2, validation_split=0.15, callbacks = [callback])\n",
    "\n",
    "# Predict the test set then turn the prediction into labels\n",
    "pred = model.predict(X_test_struct_array)\n",
    "pred_labels_structure = ohe.inverse_transform(pred)\n",
    "\n",
    "# Save scores for plotting\n",
    "f1_structure = f1_score(Y_test, pred_labels_structure, average=\"weighted\")\n",
    "prec_structure = precision_score(Y_test, pred_labels_structure, average=\"weighted\")\n",
    "rec_structure = recall_score(Y_test, pred_labels_structure, average=\"weighted\")\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(Y_test, pred_labels_structure, digits=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating the nucleotide sequence to the structural information\n",
    "\n",
    "In Multi-Omics analysis there is a concept of so called [data integration](https://www.researchgate.net/publication/358074725_Machine_learning_for_multi-omics_data_integration_in_cancer) to combine several  molecular levels (e.g. Transcriptomics, Proteomics) to train a ML classifier to detect pattern over several molecular layers. Obviously, we are not dealing with Omics datasets in this example but still the concept of data integration can be transfered to our biological problem. \n",
    "\n",
    "We would like to integrate different input levels (sequence and strcuture) into one model. This can be approached by \"early\" or \"late\" integration.\n",
    "\n",
    "Early Integration refers to the practice of combining several layer of inputs already in the beginning in our case before adding it to the model. \n",
    "\n",
    "We do that by creating an arbitrary encoding of the nucleotide sequence combined with the structural sequence information. Therefore we need to create a function that encodes the sequence the following way: Every combination of nucleotide (A, C, G, T) and structure (F, S, I, M, H, T) is encoded as its own letter. If there are any other letters we encode the combination as \"N\".\n",
    "\n",
    "||F|S|I|M|H|T|\n",
    "|---|---|---|---|---|---|---|\n",
    "|**A**|Q|W|E|R|T|Z|\n",
    "|**C**|U|I|O|P|A|S|\n",
    "|**G**|D|F|G|H|J|K|\n",
    "|**T**|L|Y|X|C|V|B|\n",
    "|**other**|N|N|N|N|N|N|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def struct_annotator(sequence, structure):\n",
    "    if (len(sequence) != len(structure)):\n",
    "        print(\"Sequence and structure not of equal length\")\n",
    "    else:\n",
    "        # Create empty string that we can append the encoding to for each sequence\n",
    "        annotated_struct = \"\"\n",
    "        \n",
    "        #Assignment of letters for combinations\n",
    "        dic = {\"A\": [\"Q\", \"W\", \"E\", \"R\", \"T\", \"Z\"],\n",
    "               \"C\": [\"U\", \"I\", \"O\", \"P\", \"A\", \"S\"],\n",
    "               \"G\": [\"D\", \"F\", \"G\", \"H\", \"J\", \"K\"],\n",
    "               \"T\": [\"L\", \"Y\", \"X\", \"C\", \"V\", \"B\"],\n",
    "               \"other\": [\"N\", \"N\", \"N\", \"N\", \"N\", \"N\"]}\n",
    "        # Save into a df, such that we can access nucleotide by column and structure by index\n",
    "        df = pd.DataFrame(dic, index = [\"F\", \"S\", \"I\", \"M\", \"H\", \"T\"])\n",
    "        \n",
    "        for i in range(0, len(sequence)):\n",
    "            \n",
    "            # Annotate according to the nucleotide\n",
    "            if (sequence[i] in [\"A\", \"C\", \"G\", \"T\"]):\n",
    "                # Append the structure with the correct letter\n",
    "                annotated_struct = annotated_struct + df[sequence[i]][structure[i]]\n",
    "                \n",
    "            # Annotate if unknown IUPAC ambiguity code is found\n",
    "            else:\n",
    "                annotated_struct = annotated_struct + df[\"other\"][structure[i]]\n",
    "    return(annotated_struct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply our function to every sequence in our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column for our sequence/structure mix\n",
    "struct_df[\"seq_struct\"] = \"\"\n",
    "\n",
    "# Fill column with our function\n",
    "for i in range(len(struct_df)):\n",
    "    struct_df[\"seq_struct\"][i] = struct_annotator(struct_df[\"sequence\"][i], struct_df[\"structure\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the same preprocessing steps from before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_df[\"seq_struct_pad\"] = struct_df[\"seq_struct\"].str.rjust(600, \"_\")  # Padding\n",
    "struct_df[\"seq_struct_pad_ord\"] = struct_df[\"seq_struct_pad\"].map(lambda x: [ord(c) for c in x])  # Ordinal Encoding\n",
    "\n",
    "ohe_seq_3 = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")  # Creating a new One-Hot Encoder\n",
    "ohe_seq_3.fit(np.array(struct_df[\"seq_struct_pad_ord\"][0]).reshape(-1, 1))  # Fit the OHE to the new input\n",
    "struct_df[\"seq_struct_ohe\"] = struct_df[\"seq_struct_pad_ord\"].map(lambda x: ohe_seq_3.transform(np.array(x).reshape(-1,1)))  # Transform to the one-hot encoded input\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(struct_df[[\"seq_struct_ohe\"]], struct_df[\"type\"], test_size=0.15, random_state=random, stratify=struct_df[\"type\"])\n",
    "\n",
    "# Transform input into arrays\n",
    "X_train_seq_struct_array = np.array(X_train[\"seq_struct_ohe\"].to_list()).reshape(len(X_train), 600, 19)  # Turn into an array\n",
    "X_test_seq_struct_array = np.array(X_test[\"seq_struct_ohe\"].to_list()).reshape(len(X_test), 600, 19)  # Turn into an array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our early integration model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Create and fit the model\n",
    "model = create_cnn_model_struct(input_dim_2=19)\n",
    "model.fit(X_train_seq_struct_array, Y_train_ohe, epochs=150, batch_size=40, verbose=2, validation_split=0.15, callbacks=[callback])\n",
    "\n",
    "# Predict test set, then transform back into labels\n",
    "pred = model.predict(X_test_seq_struct_array)\n",
    "pred_labels_early_int = ohe.inverse_transform(pred)\n",
    "\n",
    "# Save the scores for later visualization\n",
    "f1_early_int = f1_score(Y_test, pred_labels_early_int, average=\"weighted\")\n",
    "prec_early_int = precision_score(Y_test, pred_labels_early_int, average=\"weighted\")\n",
    "rec_early_int = recall_score(Y_test, pred_labels_early_int, average=\"weighted\")\n",
    "\n",
    "# Print Classification Report\n",
    "print(classification_report(Y_test, pred_labels_early_int, digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_seq_struct_array, X_test_seq_struct_array\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Late Integration is in practice the training and optimization of several models in parallel and combining the outputs of both in one layer before the final prediction. In our case the models are  concatenated at the \"head\" (before the output) and create some kind of consens prediction. The advantage is to not create a specific encoding as each input sample consists of one nucleotide sequence and one structure sequence. \n",
    "\n",
    "Within the same function, we will create two times the same models, just varying the input to be structural (annotated structure sequence) or sequence (nucleotides). Each model will have a dense ReLU layer with 10 nodes instead of the original output layer. The last layer of each model will be concatenated using keras [Concatenate](https://keras.io/api/layers/merging_layers/concatenate/) layer. After the concatenation we add the output (but it could make sense to add more dense layers to learn from the combination of the single outputs). \n",
    "\n",
    "Note, that this model has roughly twice as many parameters than the Early Integration model, so training might take longer and results will probably be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_late_integration():\n",
    "    \n",
    "    #Sequence Model\n",
    "    seq_model = keras.Sequential()\n",
    "\n",
    "    seq_model.add(keras.layers.Conv1D(128, 11, activation='relu', padding='same', input_shape=(600, 5)))\n",
    "    seq_model.add(keras.layers.MaxPooling1D(4))\n",
    "    seq_model.add(keras.layers.Dropout(rate = 0.5))\n",
    "\n",
    "    seq_model.add(keras.layers.Conv1D(64, 7, activation='relu', padding='same'))\n",
    "    seq_model.add(keras.layers.MaxPooling1D(4))\n",
    "    seq_model.add(keras.layers.Dropout(rate = 0.5))\n",
    "\n",
    "    seq_model.add(keras.layers.Flatten())\n",
    "    seq_model.add(keras.layers.Dense(10, activation='relu'))\n",
    "    \n",
    "    #Structure Model\n",
    "    struct_model = keras.Sequential()\n",
    "\n",
    "    struct_model.add(keras.layers.Conv1D(128, 11, activation='relu', padding='same', input_shape=(600, 6)))\n",
    "    struct_model.add(keras.layers.MaxPooling1D(4))\n",
    "    struct_model.add(keras.layers.Dropout(rate = 0.5))\n",
    "\n",
    "    struct_model.add(keras.layers.Conv1D(64, 7, activation='relu', padding='same'))\n",
    "    struct_model.add(keras.layers.MaxPooling1D(4))\n",
    "    struct_model.add(keras.layers.Dropout(rate = 0.5))\n",
    "\n",
    "    struct_model.add(keras.layers.Flatten())\n",
    "    struct_model.add(keras.layers.Dense(10, activation='relu'))\n",
    "    \n",
    "    #Merging the models\n",
    "    concatenated = keras.layers.Concatenate()([seq_model.output, struct_model.output])\n",
    "    concatenated = keras.layers.Dense(3, activation = \"softmax\", name = \"output_layer\")(concatenated)\n",
    "    final_model = keras.Model([seq_model.input, struct_model.input], concatenated)\n",
    "    \n",
    "    Adam = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    final_model.compile(optimizer=Adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    del struct_model, seq_model\n",
    "    return(final_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have first to create again the sequence information for the input from Notebook 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding and ordinal encoding\n",
    "struct_df[\"seq_pad\"] = struct_df[\"sequence\"].str.rjust(600, \"_\")\n",
    "struct_df[\"seq_pad_ord\"] = struct_df[\"seq_pad\"].map(lambda x: [ord(c) for c in x])\n",
    "\n",
    "# one-hot encoding\n",
    "ohe_seq_4 = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n",
    "ohe_seq_4.fit(np.array(struct_df[\"seq_pad_ord\"][0]).reshape(-1, 1))\n",
    "struct_df[\"seq_ohe\"] = struct_df[\"seq_pad_ord\"].map(lambda x: ohe_seq_4.transform(np.array(x).reshape(-1,1)))\n",
    "\n",
    "# Create train test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(struct_df[[\"seq_ohe\"]], struct_df[\"type\"], test_size=0.15, random_state=random, stratify=struct_df[\"type\"])\n",
    "\n",
    "# Transform into arrays\n",
    "X_train_seq_array = np.array(X_train[\"seq_ohe\"].to_list()).reshape(len(X_train), 600, 5)\n",
    "X_test_seq_array = np.array(X_test[\"seq_ohe\"].to_list()).reshape(len(X_test), 600, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train. Since we have the two different inputs, we enter the training and test data in a list: `[X_train_seq_array, X_train_struct_array]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Create and fit the model\n",
    "model = create_model_late_integration()\n",
    "model.fit([X_train_seq_array, X_train_struct_array], Y_train_ohe, epochs=150, batch_size=40, verbose=2, validation_split=0.15, callbacks=[callback])\n",
    "\n",
    "# Predict the test set and transform the prediction to labels\n",
    "pred = model.predict([X_test_seq_array, X_test_struct_array])\n",
    "pred_labels_late_int = ohe.inverse_transform(pred)\n",
    "\n",
    "# Save scores for plotting\n",
    "f1_late_int = f1_score(Y_test, pred_labels_late_int, average=\"weighted\")\n",
    "prec_late_int = precision_score(Y_test, pred_labels_late_int, average=\"weighted\")\n",
    "rec_late_int = recall_score(Y_test, pred_labels_late_int, average=\"weighted\")\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(Y_test, pred_labels_late_int, digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrix = confusion_matrix(Y_test, pred_labels_late_int, normalize=\"true\")\n",
    "\n",
    "# Transform into a dataframe\n",
    "conf_df = pd.DataFrame(conf_matrix, index=[\"CD-Box\", \"HACA-Box\", \"scaRNA\"], columns=[\"CD-Box\", \"HACA-Box\", \"scaRNA\"])\n",
    "\n",
    "# Plot heatmap\n",
    "sns.heatmap(conf_df, annot=True, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final visualization we can now compare the influence of the different inputs and combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary with all scores\n",
    "dictionary = {\"F1\": [f1_db, f1_structure, f1_early_int, f1_late_int],\n",
    "              \"Precision\": [prec_db, prec_structure, prec_early_int, prec_late_int],\n",
    "              \"Recall\": [rec_db, rec_structure, rec_early_int, rec_late_int],\n",
    "              \"Model\": [\"Dot-Bracket\", \"Structure\", \"Early Integration\", \"Late Integration\"]}\n",
    "\n",
    "# Save them into a dataframe\n",
    "df = pd.DataFrame(dictionary)\n",
    "df = pd.melt(df, id_vars=[\"Model\"])\n",
    "\n",
    "# Plot the scores using a barplot\n",
    "sns.barplot(data=df, x=\"variable\", y=\"value\", hue=\"Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What input seems to perform best? Is the combination/integration giving better results?**"
   ]
  }
 ],
 "metadata": {
  "jupyter_starters": {
   "starter": {
    "schema": {},
    "uiSchema": {}
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
